{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 spatula is a modern Python library for writing maintainable web scrapers. Source: https://github.com/jamesturk/spatula Documentation: https://jamesturk.github.io/spatula/ Issues: https://github.com/jamesturk/spatula/issues Features \u00b6 Page-oriented design : Encourages writing understandable & maintainable scrapers. Not Just HTML : Provides built in handlers for common data formats including CSV, JSON, XML, PDF, and Excel. Or write your own. Fast HTML parsing : Uses lxml.html for fast, consistent, and reliable parsing of HTML. Flexible Data Model Support : Compatible with dataclasses , attrs , pydantic , or bring your own data model classes for storing & validating your scraped data. CLI Tools : Offers several CLI utilities that can help streamline development & testing cycle. Fully Typed : Makes full use of Python 3 type annotations. Installation \u00b6 spatula is on PyPI, and can be installed via any standard package management tool: poetry add spatula or: pip install spatula Example \u00b6 An example of a fairly simple two-page scrape, read A First Scraper for a walkthrough of how it was built. from spatula import HtmlPage , HtmlListPage , CSS , XPath , SelectorError class EmployeeList ( HtmlListPage ): # by providing this here, it can be omitted on the command line # useful in cases where the scraper is only meant for one page source = \"https://yoyodyne-propulsion.herokuapp.com/staff\" # each row represents an employee selector = CSS ( \"#employees tbody tr\" ) def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( dict ( first = first . text , last = last . text , position = position . text , ), source = XPath ( \"./a/@href\" ) . match_one ( details ), ) def get_next_source ( self ): try : return XPath ( \"//a[contains(text(), 'Next')]/@href\" ) . match_one ( self . root ) except SelectorError : pass class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape ** self . input , ) def process_error_response ( self , exc ): self . logger . warning ( exc )","title":"Overview"},{"location":"#overview","text":"spatula is a modern Python library for writing maintainable web scrapers. Source: https://github.com/jamesturk/spatula Documentation: https://jamesturk.github.io/spatula/ Issues: https://github.com/jamesturk/spatula/issues","title":"Overview"},{"location":"#features","text":"Page-oriented design : Encourages writing understandable & maintainable scrapers. Not Just HTML : Provides built in handlers for common data formats including CSV, JSON, XML, PDF, and Excel. Or write your own. Fast HTML parsing : Uses lxml.html for fast, consistent, and reliable parsing of HTML. Flexible Data Model Support : Compatible with dataclasses , attrs , pydantic , or bring your own data model classes for storing & validating your scraped data. CLI Tools : Offers several CLI utilities that can help streamline development & testing cycle. Fully Typed : Makes full use of Python 3 type annotations.","title":"Features"},{"location":"#installation","text":"spatula is on PyPI, and can be installed via any standard package management tool: poetry add spatula or: pip install spatula","title":"Installation"},{"location":"#example","text":"An example of a fairly simple two-page scrape, read A First Scraper for a walkthrough of how it was built. from spatula import HtmlPage , HtmlListPage , CSS , XPath , SelectorError class EmployeeList ( HtmlListPage ): # by providing this here, it can be omitted on the command line # useful in cases where the scraper is only meant for one page source = \"https://yoyodyne-propulsion.herokuapp.com/staff\" # each row represents an employee selector = CSS ( \"#employees tbody tr\" ) def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( dict ( first = first . text , last = last . text , position = position . text , ), source = XPath ( \"./a/@href\" ) . match_one ( details ), ) def get_next_source ( self ): try : return XPath ( \"//a[contains(text(), 'Next')]/@href\" ) . match_one ( self . root ) except SelectorError : pass class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape ** self . input , ) def process_error_response ( self , exc ): self . logger . warning ( exc )","title":"Example"},{"location":"advanced-usage/","text":"Advanced Usage \u00b6 This section TBD. Custom Sources \u00b6 Custom Page Types \u00b6 Custom Selectors \u00b6 Page dependencies \u00b6","title":"Advanced Usage"},{"location":"advanced-usage/#advanced-usage","text":"This section TBD.","title":"Advanced Usage"},{"location":"advanced-usage/#custom-sources","text":"","title":"Custom Sources"},{"location":"advanced-usage/#custom-page-types","text":"","title":"Custom Page Types"},{"location":"advanced-usage/#custom-selectors","text":"","title":"Custom Selectors"},{"location":"advanced-usage/#page-dependencies","text":"","title":"Page dependencies"},{"location":"changelog/","text":"Changelog \u00b6 Note spatula 1.0 should be ready by Fall of 2021, providing a more stable interface to build upon, until then interfaces may change between releases. 0.8.1 - 2021-06-17 \u00b6 remove undocumented page_to_items function added Page.do_scrape to programmatically get all items from a scrape added --source parameter to scout & scrape commands 0.8.0 - 2021-06-15 \u00b6 remove undocumented Workflow allow using Page instances (as opposed to just the type) for scout & scrape add check for get_filename on output classes to override default filename improved automatic pydantic support add --timeout, --no-verify, --retries, --retry-wait options add --fastmode option to use local cache fix all CLI commands to obey various scraper options 0.7.1 - 2021-06-14 \u00b6 remove undocumented default behavior for get_source_from_input major documentation overhaul fixes for scout scrape when working with raw data returns 0.7.0 - 2021-06-04 \u00b6 add spatula scout command make error messages a bit more clear improvements to documentation added more CLI options to control verbosity, user agent, etc. if module cannot be found, search current directory 0.6.0 - 2021-04-12 \u00b6 add full typing to library small bugfixes 0.5.0 - 2021-02-04 \u00b6 add ExcelListPage improve Page.logger and CLI output move to simpler Workflow class spatula scrape can now take the name of a page, will use default Workflow bugfix: inconsistent name for process_error_response 0.4.1 - 2021-02-01 \u00b6 bugfix: dependencies are instantiated from parent page input 0.4.0 - 2021-02-01 \u00b6 restore Python 3.7 compatibility add behavior to handle returning additional Page subclasses to continue scraping add default behavior when Page.input has a url attribute. add PdfPage add page_to_items helper add Page.example_input and Page.example_source for test command add Page.logger for logging allow use of dataclasses in addition to attrs as input objects improve output of HTML elements bugfix: not specifying a page processor on workflow is no longer an error 0.3.0 - 2021-01-18 \u00b6 first documented major release major refactor, inspired by not directly using code from prior versions","title":"Changelog"},{"location":"changelog/#changelog","text":"Note spatula 1.0 should be ready by Fall of 2021, providing a more stable interface to build upon, until then interfaces may change between releases.","title":"Changelog"},{"location":"changelog/#081-2021-06-17","text":"remove undocumented page_to_items function added Page.do_scrape to programmatically get all items from a scrape added --source parameter to scout & scrape commands","title":"0.8.1 - 2021-06-17"},{"location":"changelog/#080-2021-06-15","text":"remove undocumented Workflow allow using Page instances (as opposed to just the type) for scout & scrape add check for get_filename on output classes to override default filename improved automatic pydantic support add --timeout, --no-verify, --retries, --retry-wait options add --fastmode option to use local cache fix all CLI commands to obey various scraper options","title":"0.8.0 - 2021-06-15"},{"location":"changelog/#071-2021-06-14","text":"remove undocumented default behavior for get_source_from_input major documentation overhaul fixes for scout scrape when working with raw data returns","title":"0.7.1 - 2021-06-14"},{"location":"changelog/#070-2021-06-04","text":"add spatula scout command make error messages a bit more clear improvements to documentation added more CLI options to control verbosity, user agent, etc. if module cannot be found, search current directory","title":"0.7.0 - 2021-06-04"},{"location":"changelog/#060-2021-04-12","text":"add full typing to library small bugfixes","title":"0.6.0 - 2021-04-12"},{"location":"changelog/#050-2021-02-04","text":"add ExcelListPage improve Page.logger and CLI output move to simpler Workflow class spatula scrape can now take the name of a page, will use default Workflow bugfix: inconsistent name for process_error_response","title":"0.5.0 - 2021-02-04"},{"location":"changelog/#041-2021-02-01","text":"bugfix: dependencies are instantiated from parent page input","title":"0.4.1 - 2021-02-01"},{"location":"changelog/#040-2021-02-01","text":"restore Python 3.7 compatibility add behavior to handle returning additional Page subclasses to continue scraping add default behavior when Page.input has a url attribute. add PdfPage add page_to_items helper add Page.example_input and Page.example_source for test command add Page.logger for logging allow use of dataclasses in addition to attrs as input objects improve output of HTML elements bugfix: not specifying a page processor on workflow is no longer an error","title":"0.4.0 - 2021-02-01"},{"location":"changelog/#030-2021-01-18","text":"first documented major release major refactor, inspired by not directly using code from prior versions","title":"0.3.0 - 2021-01-18"},{"location":"cli/","text":"Command Line Interface \u00b6 spatula provides a command line interface that is useful for iterative development of scrapers. Once installed within your Python environment, spatula can be invoked on the command line. E.g.: (scrape-venv) ~/scrape-proj $ spatula --version spatula, version 0.8.1 Or with poetry: ~/scrape-proj $ poetry run spatula --version spatula, version 0.8.1 The CLI provides four useful subcommands for different stages of development: spatula \u00b6 Usage: spatula [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --version boolean Show the version and exit. False --help boolean Show this message and exit. False scout \u00b6 Run first step of scrape & output data to a JSON file. This command is intended to be used to detect at a first approximation whether or not a full scrape might need to be run. If the first layer detects any changes it is safe to say that the full run will as well. This will work in the common case where a new subpage is added or removed. Of course in more advanced cases this depends upon the first page being scraped (typically a ListPage derivative) surfacing enough information (perhaps a last_updated date) to know whether any of the other pages have been scraped. Usage: spatula scout [OPTIONS] INITIAL_PAGE_NAME Options: Name Type Description Default -s , --source text Provide (or override) source URL required -o , --output-file text override default output file [default: scout.json]. scout.json -ua , --user-agent text override default user-agent spatula 0.7.1 --rpm integer set requests per minute (default: 60) 60 --timeout integer set HTTP request timeout in seconds (default: 5) 5 --verify / --no-verify boolean control verification of SSL certs True --retries integer configure how many retries to perform on HTTP request error (default: 0) 0 --retry-wait integer configure how many seconds to wait on HTTP request error (default: 10) 10 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --fastmode boolean use a cache to avoid making unnecessary requests False --help boolean Show this message and exit. False scrape \u00b6 Run full scrape, and output data to disk. Usage: spatula scrape [OPTIONS] INITIAL_PAGE_NAME Options: Name Type Description Default -o , --output-dir text override default output directory. required -s , --source text Provide (or override) source URL required -ua , --user-agent text override default user-agent spatula 0.7.1 --rpm integer set requests per minute (default: 60) 60 --timeout integer set HTTP request timeout in seconds (default: 5) 5 --verify / --no-verify boolean control verification of SSL certs True --retries integer configure how many retries to perform on HTTP request error (default: 0) 0 --retry-wait integer configure how many seconds to wait on HTTP request error (default: 10) 10 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --fastmode boolean use a cache to avoid making unnecessary requests False --help boolean Show this message and exit. False shell \u00b6 Start a session to interact with a particular page. Usage: spatula shell [OPTIONS] URL Options: Name Type Description Default -X , --verb text set HTTP verb such as POST GET -ua , --user-agent text override default user-agent spatula 0.7.1 --rpm integer set requests per minute (default: 60) 60 --timeout integer set HTTP request timeout in seconds (default: 5) 5 --verify / --no-verify boolean control verification of SSL certs True --retries integer configure how many retries to perform on HTTP request error (default: 0) 0 --retry-wait integer configure how many seconds to wait on HTTP request error (default: 10) 10 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --fastmode boolean use a cache to avoid making unnecessary requests False --help boolean Show this message and exit. False test \u00b6 Scrape a single page and see output immediately. This eases the common cycle of making modifications to a scraper, running a scrape (possibly with long-running but irrelevant portions commented out), and comparing output to what is expected. test can also be useful for debugging existing scrapers, you can see exactly what a single step of the scrape is providing, to help narrow down where erroneous data is coming from. Example: $ spatula test path.to.ClassName --source https://example.com This will run the scraper defined at path.to.ClassName against the provided URL. Usage: spatula test [OPTIONS] CLASS_NAME Options: Name Type Description Default --interactive / --no-interactive boolean Interactively prompt for missing data. False -d , --data text Provide input data in name=value pairs. required -s , --source text Provide (or override) source URL required --pagination / --no-pagination boolean Determine whether or not pagination should be followed or one page is enough for testing True -ua , --user-agent text override default user-agent spatula 0.7.1 --rpm integer set requests per minute (default: 60) 60 --timeout integer set HTTP request timeout in seconds (default: 5) 5 --verify / --no-verify boolean control verification of SSL certs True --retries integer configure how many retries to perform on HTTP request error (default: 0) 0 --retry-wait integer configure how many seconds to wait on HTTP request error (default: 10) 10 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --fastmode boolean use a cache to avoid making unnecessary requests False --help boolean Show this message and exit. False","title":"Command Line Interface"},{"location":"cli/#command-line-interface","text":"spatula provides a command line interface that is useful for iterative development of scrapers. Once installed within your Python environment, spatula can be invoked on the command line. E.g.: (scrape-venv) ~/scrape-proj $ spatula --version spatula, version 0.8.1 Or with poetry: ~/scrape-proj $ poetry run spatula --version spatula, version 0.8.1 The CLI provides four useful subcommands for different stages of development:","title":"Command Line Interface"},{"location":"cli/#spatula","text":"Usage: spatula [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --version boolean Show the version and exit. False --help boolean Show this message and exit. False","title":"spatula"},{"location":"cli/#scout","text":"Run first step of scrape & output data to a JSON file. This command is intended to be used to detect at a first approximation whether or not a full scrape might need to be run. If the first layer detects any changes it is safe to say that the full run will as well. This will work in the common case where a new subpage is added or removed. Of course in more advanced cases this depends upon the first page being scraped (typically a ListPage derivative) surfacing enough information (perhaps a last_updated date) to know whether any of the other pages have been scraped. Usage: spatula scout [OPTIONS] INITIAL_PAGE_NAME Options: Name Type Description Default -s , --source text Provide (or override) source URL required -o , --output-file text override default output file [default: scout.json]. scout.json -ua , --user-agent text override default user-agent spatula 0.7.1 --rpm integer set requests per minute (default: 60) 60 --timeout integer set HTTP request timeout in seconds (default: 5) 5 --verify / --no-verify boolean control verification of SSL certs True --retries integer configure how many retries to perform on HTTP request error (default: 0) 0 --retry-wait integer configure how many seconds to wait on HTTP request error (default: 10) 10 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --fastmode boolean use a cache to avoid making unnecessary requests False --help boolean Show this message and exit. False","title":"scout"},{"location":"cli/#scrape","text":"Run full scrape, and output data to disk. Usage: spatula scrape [OPTIONS] INITIAL_PAGE_NAME Options: Name Type Description Default -o , --output-dir text override default output directory. required -s , --source text Provide (or override) source URL required -ua , --user-agent text override default user-agent spatula 0.7.1 --rpm integer set requests per minute (default: 60) 60 --timeout integer set HTTP request timeout in seconds (default: 5) 5 --verify / --no-verify boolean control verification of SSL certs True --retries integer configure how many retries to perform on HTTP request error (default: 0) 0 --retry-wait integer configure how many seconds to wait on HTTP request error (default: 10) 10 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --fastmode boolean use a cache to avoid making unnecessary requests False --help boolean Show this message and exit. False","title":"scrape"},{"location":"cli/#shell","text":"Start a session to interact with a particular page. Usage: spatula shell [OPTIONS] URL Options: Name Type Description Default -X , --verb text set HTTP verb such as POST GET -ua , --user-agent text override default user-agent spatula 0.7.1 --rpm integer set requests per minute (default: 60) 60 --timeout integer set HTTP request timeout in seconds (default: 5) 5 --verify / --no-verify boolean control verification of SSL certs True --retries integer configure how many retries to perform on HTTP request error (default: 0) 0 --retry-wait integer configure how many seconds to wait on HTTP request error (default: 10) 10 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --fastmode boolean use a cache to avoid making unnecessary requests False --help boolean Show this message and exit. False","title":"shell"},{"location":"cli/#test","text":"Scrape a single page and see output immediately. This eases the common cycle of making modifications to a scraper, running a scrape (possibly with long-running but irrelevant portions commented out), and comparing output to what is expected. test can also be useful for debugging existing scrapers, you can see exactly what a single step of the scrape is providing, to help narrow down where erroneous data is coming from. Example: $ spatula test path.to.ClassName --source https://example.com This will run the scraper defined at path.to.ClassName against the provided URL. Usage: spatula test [OPTIONS] CLASS_NAME Options: Name Type Description Default --interactive / --no-interactive boolean Interactively prompt for missing data. False -d , --data text Provide input data in name=value pairs. required -s , --source text Provide (or override) source URL required --pagination / --no-pagination boolean Determine whether or not pagination should be followed or one page is enough for testing True -ua , --user-agent text override default user-agent spatula 0.7.1 --rpm integer set requests per minute (default: 60) 60 --timeout integer set HTTP request timeout in seconds (default: 5) 5 --verify / --no-verify boolean control verification of SSL certs True --retries integer configure how many retries to perform on HTTP request error (default: 0) 0 --retry-wait integer configure how many seconds to wait on HTTP request error (default: 10) 10 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --fastmode boolean use a cache to avoid making unnecessary requests False --help boolean Show this message and exit. False","title":"test"},{"location":"contributing/","text":"Contributing \u00b6 Issues \u00b6 Bug reports, questions, or feature requests can be submitted as GitHub issues . Developing Locally \u00b6 Before starting, you'll need poetry installed. pre-commit is also recommended. Fork spatula and check out your fork: $ git clone git@github.com:<your username>/spatula.git Install spatula and its development dependencies locally: $ cd spatula $ poetry install From here, you can use poetry run inv to run several useful maintenance commands: Install pre-commit hooks $ pre-commit install This will make sure that the linters run before each commit, saving you time. Running Tests \u00b6 poetry run inv test will run all tests and write coverage information to htmlcov/index.html Linting & Type Checking \u00b6 poetry run inv lint will run flake8 and black to lint the code style. poetry run inv mypy will run the mypy type checker. Building Docs \u00b6 poetry run inv docs will build the docs and watch for changes.","title":"Contributing"},{"location":"contributing/#contributing","text":"","title":"Contributing"},{"location":"contributing/#issues","text":"Bug reports, questions, or feature requests can be submitted as GitHub issues .","title":"Issues"},{"location":"contributing/#developing-locally","text":"Before starting, you'll need poetry installed. pre-commit is also recommended. Fork spatula and check out your fork: $ git clone git@github.com:<your username>/spatula.git Install spatula and its development dependencies locally: $ cd spatula $ poetry install From here, you can use poetry run inv to run several useful maintenance commands: Install pre-commit hooks $ pre-commit install This will make sure that the linters run before each commit, saving you time.","title":"Developing Locally"},{"location":"contributing/#running-tests","text":"poetry run inv test will run all tests and write coverage information to htmlcov/index.html","title":"Running Tests"},{"location":"contributing/#linting-type-checking","text":"poetry run inv lint will run flake8 and black to lint the code style. poetry run inv mypy will run the mypy type checker.","title":"Linting &amp; Type Checking"},{"location":"contributing/#building-docs","text":"poetry run inv docs will build the docs and watch for changes.","title":"Building Docs"},{"location":"data-models/","text":"Data Models \u00b6 Why Use Data Models? \u00b6 Back in Chaining Pages Together we saw that when chaining pages we can pass data through from the parent page. class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape, # in this case a dict we can expand here ** self . input , ) Dictionaries seem to work well for this since we can decide what data we want to grab on each page and combine them easily, but as scrapers tend to evolve over time it can be nice to have something a bit more self-documenting, and add the possibility to validate the data we're collecting. That's where we can introduce dataclasses , attrs , or pydantic models: dataclasses from dataclasses import dataclass @dataclass class Employee : first : str last : str position : str marital_status : str children : int hired : str attrs import attr @attr . s ( auto_attribs = True ) class Employee : first : str last : str position : str marital_status : str children : int hired : str pydantic from pydantic import BaseModel class Employee ( BaseModel ): first : str last : str position : str marital_status : str children : int hired : str Aren't sure which one to pick? dataclasses are built in to Python and easy to start with. You'll notice the examples barely differ, so it is easy to switch between them later on. If you want to add validation, pydantic is a great choice. And then we'll update EmployeeDetail.process_page to return our new Employee class: def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return Employee ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape, # in this case a dict we can expand here ** self . input , ) Let's give this a run: $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 Traceback (most recent call last): ... TypeError: __init__() missing 3 required positional arguments: 'first', 'last', and 'position' Of course! We're expecting self.input to contain these values, but when we're running spatula test it doesn't know what data would have been passed in. Defining input_type \u00b6 spatula provides a way to make the dependency on self.input more explicit, and restore our ability to test as a bonus side effect. Let's add a new data model that just includes the fields we're getting from the EmployeeList page: dataclasses @dataclass class PartialEmployee : first : str last : str position : str attrs @attr . s ( auto_attribs = True ) class PartialEmployee : first : str last : str position : str pydantic class PartialEmployee ( BaseModel ): first : str last : str position : str And then we'll update PersonDetail to set an input_type and stop assuming self.input is a dict : class EmployeeDetail ( HtmlPage ): input_type = PartialEmployee def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return Employee ( first = self . input . first , last = self . input . last , position = self . input . position , marital_status = marital_status . text , children = children . text , hired = hired . text , ) And now when we re-run the test command: $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" EmployeeDetail expects input (PartialEmployee): first: ~first last: ~last position: ~position INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 Employee(first='~first', last='~last', position='~position', marital_status='Married', children='1', hired='3/6/1963') Test data has been used, so even though EmployeeList didn't pass data into EmployeeDetail we can still see roughly what the data would look like if it had. Using Inheritance \u00b6 The above pattern is pretty useful & common. Often part of the data comes from one page, and the rest from another (or perhaps even more). A nice way to handle this without introducing a ton of redundancy is by setting up your models to inherit from one another: dataclasses from dataclasses import dataclass @dataclass class PartialEmployee : first : str last : str position : str @dataclass class Employee ( PartialEmployee ): marital_status : str children : int hired : str attrs import attr @attr . s ( auto_attribs = True ) class Employee : first : str last : str position : str @attr . s ( auto_attribs = True ) class PartialEmployee ( Employee ): marital_status : str children : int hired : str pydantic from pydantic import BaseModel class PartialEmployee ( BaseModel ): first : str last : str position : str class Employee ( PartialEmployee ): marital_status : str children : int hired : str Warning Be sure to remember to decorate the derived class(es) if using dataclasses or attrs . Fixing EmployeeList \u00b6 Don't forget to have EmployeeList return a PartialEmployee instance now instead of a dict : def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( PartialEmployee ( first = first . text , last = last . text , position = position . text , ), source = XPath ( \"./a/@href\" ) . match_one ( details ), ) Overriding Default Values \u00b6 Sometimes you may want to override default values (especially useful if the behavior of the second scrape varies on data from the first). via Command Line \u00b6 The --data flag to spatula test allows overriding input values with key=value pairs. $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" --data first = John --data last = Neptune EmployeeDetail expects input (PartialEmployee): first: John last: Neptune position: ~position INFO:ex03_data.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 Employee(first='John', last='Neptune', position='~position', marital_status='Married', children='1', hired='3/6/1963') Alternately, --interactive will prompt for input data. via example_input \u00b6 You can also provide the example_input attribute on the class in question. This value is assumed to be of type example_type . For example: class EmployeeDetail ( HtmlPage ): input_type = PartialEmployee example_input = PartialEmployee ( \"John\" , \"Neptune\" , \"Engineer\" ) example_source \u00b6 Like the above example_input you can define example_source to set a default value for the --source parameter when invoking spatula test class EmployeeDetail ( HtmlPage ): input_type = PartialEmployee example_input = PartialEmployee ( \"John\" , \"Neptune\" , \"Engineer\" ) example_source = \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" Warning Be sure not to confuse source with example_source . The former is used whenever the class is invoked without a source parameter, while example_source is only used when running spatula test . get_source_from_input \u00b6 It is not uncommon to want to capture a URL as part of the data and then use that URL as the next source . Let's go ahead and modify PartialEmployee to collect a URL: dataclasses @dataclass class PartialEmployee : first : str last : str position : str url : str attrs @attr . s ( auto_attribs = True ) class PartialEmployee : first : str last : str position : str url : str pydantic class PartialEmployee ( BaseModel ): first : str last : str position : str url : str And then we'll modify EmployeeList.process_item to capture this URL, and stop providing a redundant source : def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( PartialEmployee ( first = first . text , last = last . text , position = position . text , url = XPath ( \"./a/@href\" ) . match_one ( details ), ), ) And finally, add a get_source_from_input method to EmployeeDetail (as well as updating the other uses of Employee to have URL): class EmployeeDetail ( HtmlPage ): input_type = PartialEmployee example_input = PartialEmployee ( \"John\" , \"Neptune\" , \"Engineer\" , \"https://yoyodyne-propulsion.herokuapp.com/staff/1\" , ) def get_source_from_input ( self ): return self . input . url def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return Employee ( first = self . input . first , last = self . input . last , position = self . input . position , url = self . input . url , marital_status = marital_status . text , children = children . text , hired = hired . text , ) Of course, if you have a more complex situation you can do whatever you like in get_source_from_input . Data Models As Output \u00b6 When running spatula scrape data is written to disk as JSON. The exact method of obtaining that JSON varies a bit depending on what type of output you have: Raw dict : Output will match exactly. dataclasses : dataclass.asdict will be used. attrs : attr.asdict will be used to obtain a serializable representation. pydantic : the model's dict() method will be used. By default the filename will be a UUID, but if you wish to provide your own filename you can add a get_filename method to your model. Warning When providing get_filename be sure that your filenames are still unique (you may wish to still incorporate a UUID if you don't have a key you're sure is unique). spatula does not check for this, so you may overwrite data if your get_filename function does not guarantee uniqueness.","title":"Data Models"},{"location":"data-models/#data-models","text":"","title":"Data Models"},{"location":"data-models/#why-use-data-models","text":"Back in Chaining Pages Together we saw that when chaining pages we can pass data through from the parent page. class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape, # in this case a dict we can expand here ** self . input , ) Dictionaries seem to work well for this since we can decide what data we want to grab on each page and combine them easily, but as scrapers tend to evolve over time it can be nice to have something a bit more self-documenting, and add the possibility to validate the data we're collecting. That's where we can introduce dataclasses , attrs , or pydantic models: dataclasses from dataclasses import dataclass @dataclass class Employee : first : str last : str position : str marital_status : str children : int hired : str attrs import attr @attr . s ( auto_attribs = True ) class Employee : first : str last : str position : str marital_status : str children : int hired : str pydantic from pydantic import BaseModel class Employee ( BaseModel ): first : str last : str position : str marital_status : str children : int hired : str Aren't sure which one to pick? dataclasses are built in to Python and easy to start with. You'll notice the examples barely differ, so it is easy to switch between them later on. If you want to add validation, pydantic is a great choice. And then we'll update EmployeeDetail.process_page to return our new Employee class: def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return Employee ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape, # in this case a dict we can expand here ** self . input , ) Let's give this a run: $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 Traceback (most recent call last): ... TypeError: __init__() missing 3 required positional arguments: 'first', 'last', and 'position' Of course! We're expecting self.input to contain these values, but when we're running spatula test it doesn't know what data would have been passed in.","title":"Why Use Data Models?"},{"location":"data-models/#defining-input_type","text":"spatula provides a way to make the dependency on self.input more explicit, and restore our ability to test as a bonus side effect. Let's add a new data model that just includes the fields we're getting from the EmployeeList page: dataclasses @dataclass class PartialEmployee : first : str last : str position : str attrs @attr . s ( auto_attribs = True ) class PartialEmployee : first : str last : str position : str pydantic class PartialEmployee ( BaseModel ): first : str last : str position : str And then we'll update PersonDetail to set an input_type and stop assuming self.input is a dict : class EmployeeDetail ( HtmlPage ): input_type = PartialEmployee def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return Employee ( first = self . input . first , last = self . input . last , position = self . input . position , marital_status = marital_status . text , children = children . text , hired = hired . text , ) And now when we re-run the test command: $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" EmployeeDetail expects input (PartialEmployee): first: ~first last: ~last position: ~position INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 Employee(first='~first', last='~last', position='~position', marital_status='Married', children='1', hired='3/6/1963') Test data has been used, so even though EmployeeList didn't pass data into EmployeeDetail we can still see roughly what the data would look like if it had.","title":"Defining input_type"},{"location":"data-models/#using-inheritance","text":"The above pattern is pretty useful & common. Often part of the data comes from one page, and the rest from another (or perhaps even more). A nice way to handle this without introducing a ton of redundancy is by setting up your models to inherit from one another: dataclasses from dataclasses import dataclass @dataclass class PartialEmployee : first : str last : str position : str @dataclass class Employee ( PartialEmployee ): marital_status : str children : int hired : str attrs import attr @attr . s ( auto_attribs = True ) class Employee : first : str last : str position : str @attr . s ( auto_attribs = True ) class PartialEmployee ( Employee ): marital_status : str children : int hired : str pydantic from pydantic import BaseModel class PartialEmployee ( BaseModel ): first : str last : str position : str class Employee ( PartialEmployee ): marital_status : str children : int hired : str Warning Be sure to remember to decorate the derived class(es) if using dataclasses or attrs .","title":"Using Inheritance"},{"location":"data-models/#fixing-employeelist","text":"Don't forget to have EmployeeList return a PartialEmployee instance now instead of a dict : def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( PartialEmployee ( first = first . text , last = last . text , position = position . text , ), source = XPath ( \"./a/@href\" ) . match_one ( details ), )","title":"Fixing EmployeeList"},{"location":"data-models/#overriding-default-values","text":"Sometimes you may want to override default values (especially useful if the behavior of the second scrape varies on data from the first).","title":"Overriding Default Values"},{"location":"data-models/#via-command-line","text":"The --data flag to spatula test allows overriding input values with key=value pairs. $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" --data first = John --data last = Neptune EmployeeDetail expects input (PartialEmployee): first: John last: Neptune position: ~position INFO:ex03_data.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 Employee(first='John', last='Neptune', position='~position', marital_status='Married', children='1', hired='3/6/1963') Alternately, --interactive will prompt for input data.","title":"via Command Line"},{"location":"data-models/#via-example_input","text":"You can also provide the example_input attribute on the class in question. This value is assumed to be of type example_type . For example: class EmployeeDetail ( HtmlPage ): input_type = PartialEmployee example_input = PartialEmployee ( \"John\" , \"Neptune\" , \"Engineer\" )","title":"via example_input"},{"location":"data-models/#example_source","text":"Like the above example_input you can define example_source to set a default value for the --source parameter when invoking spatula test class EmployeeDetail ( HtmlPage ): input_type = PartialEmployee example_input = PartialEmployee ( \"John\" , \"Neptune\" , \"Engineer\" ) example_source = \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" Warning Be sure not to confuse source with example_source . The former is used whenever the class is invoked without a source parameter, while example_source is only used when running spatula test .","title":"example_source"},{"location":"data-models/#get_source_from_input","text":"It is not uncommon to want to capture a URL as part of the data and then use that URL as the next source . Let's go ahead and modify PartialEmployee to collect a URL: dataclasses @dataclass class PartialEmployee : first : str last : str position : str url : str attrs @attr . s ( auto_attribs = True ) class PartialEmployee : first : str last : str position : str url : str pydantic class PartialEmployee ( BaseModel ): first : str last : str position : str url : str And then we'll modify EmployeeList.process_item to capture this URL, and stop providing a redundant source : def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( PartialEmployee ( first = first . text , last = last . text , position = position . text , url = XPath ( \"./a/@href\" ) . match_one ( details ), ), ) And finally, add a get_source_from_input method to EmployeeDetail (as well as updating the other uses of Employee to have URL): class EmployeeDetail ( HtmlPage ): input_type = PartialEmployee example_input = PartialEmployee ( \"John\" , \"Neptune\" , \"Engineer\" , \"https://yoyodyne-propulsion.herokuapp.com/staff/1\" , ) def get_source_from_input ( self ): return self . input . url def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return Employee ( first = self . input . first , last = self . input . last , position = self . input . position , url = self . input . url , marital_status = marital_status . text , children = children . text , hired = hired . text , ) Of course, if you have a more complex situation you can do whatever you like in get_source_from_input .","title":"get_source_from_input"},{"location":"data-models/#data-models-as-output","text":"When running spatula scrape data is written to disk as JSON. The exact method of obtaining that JSON varies a bit depending on what type of output you have: Raw dict : Output will match exactly. dataclasses : dataclass.asdict will be used. attrs : attr.asdict will be used to obtain a serializable representation. pydantic : the model's dict() method will be used. By default the filename will be a UUID, but if you wish to provide your own filename you can add a get_filename method to your model. Warning When providing get_filename be sure that your filenames are still unique (you may wish to still incorporate a UUID if you don't have a key you're sure is unique). spatula does not check for this, so you may overwrite data if your get_filename function does not guarantee uniqueness.","title":"Data Models As Output"},{"location":"philosophy/","text":"Design Philosophy \u00b6 spatula came out of several years of maintaining a very large, community-driven scraper project, Open States . While several of the decisions made are best understood through this lens, the goal of spatula is to remain lightweight so that even simple projects can benefit from its structure. Common Problems \u00b6 If you've written a few web scrapers, you're likely to have run into a few common issues: Scrapers are inherently messy as they tend to reflect the cleanliness (or lack thereof) of the underlying site. Scrapers are often written and then left alone for months or years, only needing maintenance when the underlying site changes. Some scrapes take hours to complete, making the development cycle for testing changes and fixes quite difficult. Goals \u00b6 The framework should make it as easy as possible to get started and write clean scraper code. While it is often easy, and tempting, to write a scraper as a dirty one-off script, spatula makes an attempt to provide an easy framework that most scrapers fit within without additional overhead. This reflects the reality that many scraper projects start small but grow quickly, so reaching for a heavyweight tool from the start often does not seem practical. The initial overhead imposed by the framework should be as light as possible, providing benefits even for authors that do not wish to use every feature available to them. The framework should make it easy to read code that was written, with as many of the underlying assumptions of the scraper presented as clearly as possible for future maintenance. By encouraging users to structure their scrapers in a readable way, scrapers will be easier to read later, and issues/maintenance can often be identified with specific components instead of forcing maintainers to comprehend a single gnarly script. Iterative development of scrapers should be the norm. It should be possible to test changes to a single part of the scrape without running the entire process. The user shouldn't have to comment out code or add temporary debug statements to do something that is extremely common when authoring scrapers. Leverage modern Python to improve the experience. There are some great things about Python 3 that we can leverage to make writing & maintaining scrapers easier. To that end, spatula is fully type-annotated, and integrates well with dataclasses , attrs , and pydantic . Pages & Page Roles \u00b6 A key component of using spatula is thinking of the scraper in terms of types of \"pages\". For each type of page you encounter, you'll write a subclass of Page to extract the data from it. Tip If you're familiar with MVC frameworks, a good way to think of this concept is the inverse of a view: a Page takes some kind of presentation (e.g. an HTML page or CSV file) and converts it back to the underlying data that it is comprised of. There are a few types of pages we generally encounter when scraping sites. How we write and use our Page subclasses will depend upon which of these four roles the page fulfills. The two most common are what we'll call \"listing\" and \"detail\" pages. Listing pages contain some kind of list of information, perhaps all of the members of a given legislature. They'll often provide links to our second type of page, which we'll call a \"detail\" page. A \"detail\" page in our example would contain additional information on a single legislator. (TODO: revisit this section later) There are two other roles that a page can play, we'll get into these in more detail in advanced topics. One is what we'll call an 'auxiliary detail' page, essentially a subpage of a detail page that contains additional information we need to collect. (An example would be if each detail page had a subpage listing a legislator's committee assignments that we wanted to collect.) The final role is what we call 'augmentation' pages, which provide data with which we want to augment the entire data set. (An example would be a separate page that is a photo directory of all legislators. We'll need to scrape this and match the photos to the legislators we scraped from the listing/detail pages.) Why don't page objects fetch themselves? \u00b6 A big part of this is that we want to share some state among them for rate limiting and other configuration yet to come. In theory this could mean that each one is instantiated with a session parameter or similar, but that'd be a bit more cumbersome. This also can make testing even easier, as there's no need to mock HTTP requests/etc. as just passing a mock object into the Page will suffice.","title":"Design Philosophy"},{"location":"philosophy/#design-philosophy","text":"spatula came out of several years of maintaining a very large, community-driven scraper project, Open States . While several of the decisions made are best understood through this lens, the goal of spatula is to remain lightweight so that even simple projects can benefit from its structure.","title":"Design Philosophy"},{"location":"philosophy/#common-problems","text":"If you've written a few web scrapers, you're likely to have run into a few common issues: Scrapers are inherently messy as they tend to reflect the cleanliness (or lack thereof) of the underlying site. Scrapers are often written and then left alone for months or years, only needing maintenance when the underlying site changes. Some scrapes take hours to complete, making the development cycle for testing changes and fixes quite difficult.","title":"Common Problems"},{"location":"philosophy/#goals","text":"The framework should make it as easy as possible to get started and write clean scraper code. While it is often easy, and tempting, to write a scraper as a dirty one-off script, spatula makes an attempt to provide an easy framework that most scrapers fit within without additional overhead. This reflects the reality that many scraper projects start small but grow quickly, so reaching for a heavyweight tool from the start often does not seem practical. The initial overhead imposed by the framework should be as light as possible, providing benefits even for authors that do not wish to use every feature available to them. The framework should make it easy to read code that was written, with as many of the underlying assumptions of the scraper presented as clearly as possible for future maintenance. By encouraging users to structure their scrapers in a readable way, scrapers will be easier to read later, and issues/maintenance can often be identified with specific components instead of forcing maintainers to comprehend a single gnarly script. Iterative development of scrapers should be the norm. It should be possible to test changes to a single part of the scrape without running the entire process. The user shouldn't have to comment out code or add temporary debug statements to do something that is extremely common when authoring scrapers. Leverage modern Python to improve the experience. There are some great things about Python 3 that we can leverage to make writing & maintaining scrapers easier. To that end, spatula is fully type-annotated, and integrates well with dataclasses , attrs , and pydantic .","title":"Goals"},{"location":"philosophy/#pages-page-roles","text":"A key component of using spatula is thinking of the scraper in terms of types of \"pages\". For each type of page you encounter, you'll write a subclass of Page to extract the data from it. Tip If you're familiar with MVC frameworks, a good way to think of this concept is the inverse of a view: a Page takes some kind of presentation (e.g. an HTML page or CSV file) and converts it back to the underlying data that it is comprised of. There are a few types of pages we generally encounter when scraping sites. How we write and use our Page subclasses will depend upon which of these four roles the page fulfills. The two most common are what we'll call \"listing\" and \"detail\" pages. Listing pages contain some kind of list of information, perhaps all of the members of a given legislature. They'll often provide links to our second type of page, which we'll call a \"detail\" page. A \"detail\" page in our example would contain additional information on a single legislator. (TODO: revisit this section later) There are two other roles that a page can play, we'll get into these in more detail in advanced topics. One is what we'll call an 'auxiliary detail' page, essentially a subpage of a detail page that contains additional information we need to collect. (An example would be if each detail page had a subpage listing a legislator's committee assignments that we wanted to collect.) The final role is what we call 'augmentation' pages, which provide data with which we want to augment the entire data set. (An example would be a separate page that is a photo directory of all legislators. We'll need to scrape this and match the photos to the legislators we scraped from the listing/detail pages.)","title":"Pages &amp; Page Roles"},{"location":"philosophy/#why-dont-page-objects-fetch-themselves","text":"A big part of this is that we want to share some state among them for rate limiting and other configuration yet to come. In theory this could mean that each one is instantiated with a session parameter or similar, but that'd be a bit more cumbersome. This also can make testing even easier, as there's no need to mock HTTP requests/etc. as just passing a mock object into the Page will suffice.","title":"Why don't page objects fetch themselves?"},{"location":"reference/","text":"API Reference \u00b6 Pages \u00b6 Page \u00b6 Base class for all Page scrapers, used for scraping information from a single type of page. Attributes source Can be set on subclasses of Page to define the initial HTTP request that the page will handle in its process_response method. For simple GET requests, source can be a string. URL should be used for more advanced use cases. response requests.Response object available if access is needed to the raw response for any reason. input Instance of data being passed upon instantiation of this page. Must be of type input_type . input_type dataclass , attrs class, or pydantic model. If set will be used to prompt for and/or validate self.input example_input Instance of input_type to be used when invoking spatula test . example_source Source to fetch when invokking spatula test . dependencies TODO: document Methods do_scrape ( self , scraper = None ) \u00b6 yield results from this page and any subpages Parameters: Name Type Description Default scraper Optional[scrapelib.Scraper] Optional scrapelib.Scraper instance to use for running scrape. None Returns: Type Description Iterable[Any] Generator yielding results from the scrape. get_next_source ( self ) \u00b6 To be overriden for paginated pages. Return a URL or valid source to fetch the next page, None if there isn't one. get_source_from_input ( self ) \u00b6 To be overridden. Convert self.input to a Source object such as a URL . postprocess_response ( self ) \u00b6 To be overridden. This is called after source.get_response but before self.process_page. process_error_response ( self , exception ) \u00b6 To be overridden. This is called after source.get_response if an exception is raised. process_page ( self ) \u00b6 To be overridden. Return data extracted from this page and this page alone. HtmlPage \u00b6 Page that automatically handles parsing and normalizing links in an HTML response. Attributes root lxml.etree.Element object representing the root element (e.g. <html> ) on the page. Can use the normal lxml methods (such as cssselect and getchildren ), or use this element as the target of a Selector subclass. JsonPage \u00b6 Page that automatically handles parsing a JSON response. Attributes data JSON data from response. (same as self.response.json() ) PdfPage \u00b6 Page that automatically handles converting a PDF response to text using pdftotext . Attributes preserve_layout set to True on derived class if you want the conversion function to use pdftotext's -layout option to attempt to preserve the layout of text. ( False by default) text UTF8 text extracted by pdftotext. XmlPage \u00b6 Page that automatically handles parsing a XML response. Attributes root lxml.etree.Element object representing the root XML element on the page. ListPages \u00b6 ListPage \u00b6 Base class for common pattern of extracting many homogenous items from one page. Instead of overriding process_response , subclasses should provide a process_item . Methods process_item ( self , item ) \u00b6 To be overridden. Called once per subitem on page, as defined by the particular subclass being used. skip ( self , msg = '' ) \u00b6 Can be called from within process_item to skip a given item. Typically used if there is some known bad data. CsvListPage \u00b6 Processes each row in a CSV (after the first, assumed to be headers) as an item with process_item . ExcelListPage \u00b6 Processes each row in an Excel file as an item with process_item . HtmlListPage \u00b6 Selects homogenous items from HTML page using selector and passes them to process_item . Attributes selector Selector subclass which matches list of homogenous elements to process. (e.g. CSS(\"tbody tr\") ) JsonListPage \u00b6 Processes each element in a JSON list as an item with process_item . XmlListPage \u00b6 Selects homogenous items from XML document using selector and passes them to process_item . Attributes selector Selector subclass which matches list of homogenous elements to process. (e.g. XPath(\"//item\") ) Selectors \u00b6 Selector \u00b6 Base class implementing Selector interface. match ( self , element , * , min_items = None , max_items = None , num_items = None ) \u00b6 Return all matches of the given selector within element . If the number of elements matched is outside the prescribed boundaries, a SelectorError is raised. Parameters: Name Type Description Default element _Element The element to match within. When used from within a Page will usually be self.root . required min_items Optional[int] A minimum number of items to match. None max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None match_one ( self , element ) \u00b6 Return exactly one match. Parameters: Name Type Description Default element _Element Element to search within. required CSS \u00b6 Utilize CSS-style selectors. Parameters: Name Type Description Default css_selector str CSS selector expression to use. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None SimilarLink \u00b6 Match links that fit a provided pattern. Parameters: Name Type Description Default pattern str Regular expression for link hrefs. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None XPath \u00b6 Utilize XPath selectors. Parameters: Name Type Description Default xpath str XPath expression to use. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None Sources \u00b6 URL \u00b6 Defines a resource to fetch via URL, particularly useful for handling non-GET requests. Parameters: Name Type Description Default url str URL to fetch required method str HTTP method to use, defaults to \"GET\" 'GET' data dict POST data to include in request body. None headers dict dictionary of HTTP headers to set for the request. None NullSource \u00b6 Special class to set as a page's source to indicate no HTTP request needs to be performed.","title":"API Reference"},{"location":"reference/#api-reference","text":"","title":"API Reference"},{"location":"reference/#pages","text":"","title":"Pages"},{"location":"reference/#page","text":"Base class for all Page scrapers, used for scraping information from a single type of page. Attributes source Can be set on subclasses of Page to define the initial HTTP request that the page will handle in its process_response method. For simple GET requests, source can be a string. URL should be used for more advanced use cases. response requests.Response object available if access is needed to the raw response for any reason. input Instance of data being passed upon instantiation of this page. Must be of type input_type . input_type dataclass , attrs class, or pydantic model. If set will be used to prompt for and/or validate self.input example_input Instance of input_type to be used when invoking spatula test . example_source Source to fetch when invokking spatula test . dependencies TODO: document Methods","title":"Page"},{"location":"reference/#spatula.pages.Page.do_scrape","text":"yield results from this page and any subpages Parameters: Name Type Description Default scraper Optional[scrapelib.Scraper] Optional scrapelib.Scraper instance to use for running scrape. None Returns: Type Description Iterable[Any] Generator yielding results from the scrape.","title":"do_scrape()"},{"location":"reference/#spatula.pages.Page.get_next_source","text":"To be overriden for paginated pages. Return a URL or valid source to fetch the next page, None if there isn't one.","title":"get_next_source()"},{"location":"reference/#spatula.pages.Page.get_source_from_input","text":"To be overridden. Convert self.input to a Source object such as a URL .","title":"get_source_from_input()"},{"location":"reference/#spatula.pages.Page.postprocess_response","text":"To be overridden. This is called after source.get_response but before self.process_page.","title":"postprocess_response()"},{"location":"reference/#spatula.pages.Page.process_error_response","text":"To be overridden. This is called after source.get_response if an exception is raised.","title":"process_error_response()"},{"location":"reference/#spatula.pages.Page.process_page","text":"To be overridden. Return data extracted from this page and this page alone.","title":"process_page()"},{"location":"reference/#htmlpage","text":"Page that automatically handles parsing and normalizing links in an HTML response. Attributes root lxml.etree.Element object representing the root element (e.g. <html> ) on the page. Can use the normal lxml methods (such as cssselect and getchildren ), or use this element as the target of a Selector subclass.","title":"HtmlPage"},{"location":"reference/#jsonpage","text":"Page that automatically handles parsing a JSON response. Attributes data JSON data from response. (same as self.response.json() )","title":"JsonPage"},{"location":"reference/#pdfpage","text":"Page that automatically handles converting a PDF response to text using pdftotext . Attributes preserve_layout set to True on derived class if you want the conversion function to use pdftotext's -layout option to attempt to preserve the layout of text. ( False by default) text UTF8 text extracted by pdftotext.","title":"PdfPage"},{"location":"reference/#xmlpage","text":"Page that automatically handles parsing a XML response. Attributes root lxml.etree.Element object representing the root XML element on the page.","title":"XmlPage"},{"location":"reference/#listpages","text":"","title":"ListPages"},{"location":"reference/#listpage","text":"Base class for common pattern of extracting many homogenous items from one page. Instead of overriding process_response , subclasses should provide a process_item . Methods","title":"ListPage"},{"location":"reference/#spatula.pages.ListPage.process_item","text":"To be overridden. Called once per subitem on page, as defined by the particular subclass being used.","title":"process_item()"},{"location":"reference/#spatula.pages.ListPage.skip","text":"Can be called from within process_item to skip a given item. Typically used if there is some known bad data.","title":"skip()"},{"location":"reference/#csvlistpage","text":"Processes each row in a CSV (after the first, assumed to be headers) as an item with process_item .","title":"CsvListPage"},{"location":"reference/#excellistpage","text":"Processes each row in an Excel file as an item with process_item .","title":"ExcelListPage"},{"location":"reference/#htmllistpage","text":"Selects homogenous items from HTML page using selector and passes them to process_item . Attributes selector Selector subclass which matches list of homogenous elements to process. (e.g. CSS(\"tbody tr\") )","title":"HtmlListPage"},{"location":"reference/#jsonlistpage","text":"Processes each element in a JSON list as an item with process_item .","title":"JsonListPage"},{"location":"reference/#xmllistpage","text":"Selects homogenous items from XML document using selector and passes them to process_item . Attributes selector Selector subclass which matches list of homogenous elements to process. (e.g. XPath(\"//item\") )","title":"XmlListPage"},{"location":"reference/#selectors","text":"","title":"Selectors"},{"location":"reference/#selector","text":"Base class implementing Selector interface.","title":"Selector"},{"location":"reference/#spatula.selectors.Selector.match","text":"Return all matches of the given selector within element . If the number of elements matched is outside the prescribed boundaries, a SelectorError is raised. Parameters: Name Type Description Default element _Element The element to match within. When used from within a Page will usually be self.root . required min_items Optional[int] A minimum number of items to match. None max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None","title":"match()"},{"location":"reference/#spatula.selectors.Selector.match_one","text":"Return exactly one match. Parameters: Name Type Description Default element _Element Element to search within. required","title":"match_one()"},{"location":"reference/#css","text":"Utilize CSS-style selectors. Parameters: Name Type Description Default css_selector str CSS selector expression to use. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None","title":"CSS"},{"location":"reference/#similarlink","text":"Match links that fit a provided pattern. Parameters: Name Type Description Default pattern str Regular expression for link hrefs. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None","title":"SimilarLink"},{"location":"reference/#xpath","text":"Utilize XPath selectors. Parameters: Name Type Description Default xpath str XPath expression to use. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None","title":"XPath"},{"location":"reference/#sources","text":"","title":"Sources"},{"location":"reference/#url","text":"Defines a resource to fetch via URL, particularly useful for handling non-GET requests. Parameters: Name Type Description Default url str URL to fetch required method str HTTP method to use, defaults to \"GET\" 'GET' data dict POST data to include in request body. None headers dict dictionary of HTTP headers to set for the request. None","title":"URL"},{"location":"reference/#nullsource","text":"Special class to set as a page's source to indicate no HTTP request needs to be performed.","title":"NullSource"},{"location":"scraper-basics/","text":"A First Scraper \u00b6 This guide contains quick examples of how you could scrape a small list of employees of the fictional Yoyodyne Propulsion Systems , a site developed for demonstrating web scraping. This will give you an idea of what it looks like to write a scraper using spatula . Scraping a List Page \u00b6 It is fairly common for a scrape to begin on some sort of directory or listing page. We'll start by scraping the staff list on https://yoyodyne-propulsion.herokuapp.com/staff This page has a fairly simple HTML table with four columns: < table id = \"employees\" > < thead > < tr > < th > First Name </ th > < th > Last Name </ th > < th > Position Name </ th > < th > &nbsp; </ th > </ tr > </ thead > < tbody > < tr > < td > John </ td > < td > Barnett </ td > < td > Scheduling </ td > < td >< a href = \"/staff/52\" > Details </ a ></ td > </ tr > < tr > < td > John </ td > < td > Bigboot\u00e9 </ td > < td > Executive Vice President </ td > < td >< a href = \"/staff/2\" > Details </ a ></ td > </ tr > ...continues... spatula provides a special interface for these cases. See below how we process each matching link by deriving from a HtmlListPage and providing a selector as well as a process_item method. Open a file named quickstart.py and add the following code: # imports we'll use in this example from spatula import ( HtmlPage , HtmlListPage , CSS , XPath , SelectorError ) class EmployeeList ( HtmlListPage ): source = \"https://yoyodyne-propulsion.herokuapp.com/staff\" # each row represents an employee selector = CSS ( \"#employees tr\" ) def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return dict ( first = first . text , last = last . text , position = position . text , ) One concept in spatula is that we typically write one class per type of page we encounter. This class defines the logic to process the employee list page. This class will turn each row on the page into a dictionary with the 'first', 'last', and 'position' keys. It can be tested from the command line like: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: {'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} 2: {'first': 'John', 'last': 'Bigboot\u00e9', 'position': 'Executive Vice President'} 3: {'first': 'John', 'last': 'Camp', 'position': 'Human Resources'} ... 10: {'first': 'John', 'last': 'Fish', 'position': 'Marine R&D'} The spatula test command lets us quickly see the output of the part of the scraper we're working on. You may notice that we're only grabbing the first page for now, we'll come back in a bit to handle pagination. Scraping a Single Page \u00b6 Employees have a few more details not included in the table on pages like https://yoyodyne-propulsion.herokuapp.com/staff/52 . We're going to pull some data elements from the page that look like: < h2 class = \"section\" > Employee Details for John Barnett </ h2 > < div class = \"section\" > < dl > < dt > Position </ dt > < dd id = \"position\" > Scheduling </ dd > < dt > Marital Status </ dt > < dd id = \"status\" > Married </ dd > < dt > Number of Children </ dt > < dd id = \"children\" > 1 </ dd > < dt > Hired </ dt > < dd id = \"hired\" > 3/6/1963 </ dd > </ dl > </ div > To demonstrate extracting the details from this page, we'll write a small class to handle individual employee pages. Whereas before we used HtmlListPage and overrode process_item , this time we'll subclass HtmlPage , and override the process_page function. class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , ) This will extract the elements from the page and return them in a dictionary. It can be tested from the command line like: $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 {'children': '1', 'hired': '3/6/1963', 'marital_status': 'Married'} One thing to note is that since we didn't define a single source attribute like we did in EmployeeList , we need to pass one on the command line with --source . This lets you quickly try your scraper against multiple variants of a page as needed. Chaining Pages Together \u00b6 Most moderately complex sites will require chaining data together from multiple pages to get a complete object. Let's revisit EmployeeList and have it return instances of EmployeeDetail to tell spatula that more work is needed: class EmployeeList ( HtmlListPage ): # by providing this here, it can be omitted on the command line # useful in cases where the scraper is only meant for one page source = \"https://yoyodyne-propulsion.herokuapp.com/staff\" # each row represents an employee selector = CSS ( \"#employees tbody tr\" ) def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( dict ( first = first . text , last = last . text , position = position . text , ), source = XPath ( \"./a/@href\" ) . match_one ( details ), ) And we can revisit EmployeeDetail to tell it to combine the data it collects with the data passed in from the parent page: class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape, # in this case a dict we can expand here ** self . input , ) Now a run looks like: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: EmployeeDetail(input={'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} source=https://yoyodyne-propulsion.herokuapp.com/staff) 2: EmployeeDetail(input={'first': 'John', 'last': 'Bigboot\u00e9', 'position': 'Executive Vice President'} source=https:/yoyodyne-propulsion.herokuapp.com/staff/2) ... 10: EmployeeDetail(input={'first': 'John', 'last': 'Fish', 'position': 'Marine R&D'} source=https:/yoyodyne-propulsion.herokuapp.com/staff/20) By default, spatula test just shows the result of the page you're working on, but you can see that it is now returning page objects with the data and a source set. Running a Scrape \u00b6 Now that we're happy with our individual page scrapers, we can run the full scrape and write the data to disk. For this we use the spatula scrape command: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/2 ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/100 INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/101 success: wrote 10 objects to _scrapes/2021-06-03/001 And now our scraped data is on disk, ready for you to use! If you look at a data file you'll see that it has the full data for an individual: { \"marital_status\" : \"Single\" , \"children\" : \"0\" , \"hired\" : \"9/9/1963\" , \"first\" : \"John\" , \"last\" : \"Omar\" , \"position\" : \"Imports & Exports\" } Using spatula Within Other Scripts \u00b6 Perhaps you don't want to write your output to disk. If you want to post-process your data further or use it as part of a larger pipeline a page's do_scrape method lets you do just that. It returns a generator that you can use to process items as you see fit. For example: page = EmployeeList () for e in page . do_scrape (): print ( e ) You an do whatever you wish with these results, output them in a custom format, save them to your database, etc. Pagination \u00b6 While writing the list page we ignored pagination, let's go ahead and add it now. If we override the get_next_source method on our EmployeeLList class, spatula will continue to the next page once it has called process_item on all elements on the current page. # add this within EmployeeList def get_next_source ( self ): try : return XPath ( \"//a[contains(text(), 'Next')]/@href\" ) . match_one ( self . root ) except SelectorError : pass You'll notice the output of spatula test quickstart.EmployeeList has now changed: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: {'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} ... paginating for EmployeeList source=https://yoyodyne-propulsion.herokuapp.com/staff?page=2 INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff?page=2 ... 45: EmployeeDetail(input={'first': 'John', 'last': 'Ya Ya', 'position': 'Computer Design Specialist'} source=https://yoyodyne-propulsion.herokuapp.com/staff/101) Error Handling \u00b6 Now that we're grabbing all 45 employees, kick off another full scrape: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/404 Traceback (most recent call last): ... scrapelib.HTTPError: 404 while retrieving https://yoyodyne-propulsion.herokuapp.com/staff/404 An error! It turns out that one of the employee pages isn't loading correctly. Sometimes it is best to let these errors propagate so you can try to fix the broken scraper. Other times it makes more sense to handle the error and move on. If you wish to do that, you can override process_error_response . Add the following to EmployeeDetail : def process_error_response ( self , exception ): # every Page subclass has a built-in logger object self . logger . warning ( exception ) Run the scrape again to see this in action: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff ... WARNING:quickstart.EmployeeDetail:404 while retrieving https://yoyodyne-propulsion.herokuapp.com/staff/404 ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/101 success: wrote 44 objects to _scrapes/2021-06-03/002","title":"A First Scraper"},{"location":"scraper-basics/#a-first-scraper","text":"This guide contains quick examples of how you could scrape a small list of employees of the fictional Yoyodyne Propulsion Systems , a site developed for demonstrating web scraping. This will give you an idea of what it looks like to write a scraper using spatula .","title":"A First Scraper"},{"location":"scraper-basics/#scraping-a-list-page","text":"It is fairly common for a scrape to begin on some sort of directory or listing page. We'll start by scraping the staff list on https://yoyodyne-propulsion.herokuapp.com/staff This page has a fairly simple HTML table with four columns: < table id = \"employees\" > < thead > < tr > < th > First Name </ th > < th > Last Name </ th > < th > Position Name </ th > < th > &nbsp; </ th > </ tr > </ thead > < tbody > < tr > < td > John </ td > < td > Barnett </ td > < td > Scheduling </ td > < td >< a href = \"/staff/52\" > Details </ a ></ td > </ tr > < tr > < td > John </ td > < td > Bigboot\u00e9 </ td > < td > Executive Vice President </ td > < td >< a href = \"/staff/2\" > Details </ a ></ td > </ tr > ...continues... spatula provides a special interface for these cases. See below how we process each matching link by deriving from a HtmlListPage and providing a selector as well as a process_item method. Open a file named quickstart.py and add the following code: # imports we'll use in this example from spatula import ( HtmlPage , HtmlListPage , CSS , XPath , SelectorError ) class EmployeeList ( HtmlListPage ): source = \"https://yoyodyne-propulsion.herokuapp.com/staff\" # each row represents an employee selector = CSS ( \"#employees tr\" ) def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return dict ( first = first . text , last = last . text , position = position . text , ) One concept in spatula is that we typically write one class per type of page we encounter. This class defines the logic to process the employee list page. This class will turn each row on the page into a dictionary with the 'first', 'last', and 'position' keys. It can be tested from the command line like: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: {'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} 2: {'first': 'John', 'last': 'Bigboot\u00e9', 'position': 'Executive Vice President'} 3: {'first': 'John', 'last': 'Camp', 'position': 'Human Resources'} ... 10: {'first': 'John', 'last': 'Fish', 'position': 'Marine R&D'} The spatula test command lets us quickly see the output of the part of the scraper we're working on. You may notice that we're only grabbing the first page for now, we'll come back in a bit to handle pagination.","title":"Scraping a List Page"},{"location":"scraper-basics/#scraping-a-single-page","text":"Employees have a few more details not included in the table on pages like https://yoyodyne-propulsion.herokuapp.com/staff/52 . We're going to pull some data elements from the page that look like: < h2 class = \"section\" > Employee Details for John Barnett </ h2 > < div class = \"section\" > < dl > < dt > Position </ dt > < dd id = \"position\" > Scheduling </ dd > < dt > Marital Status </ dt > < dd id = \"status\" > Married </ dd > < dt > Number of Children </ dt > < dd id = \"children\" > 1 </ dd > < dt > Hired </ dt > < dd id = \"hired\" > 3/6/1963 </ dd > </ dl > </ div > To demonstrate extracting the details from this page, we'll write a small class to handle individual employee pages. Whereas before we used HtmlListPage and overrode process_item , this time we'll subclass HtmlPage , and override the process_page function. class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , ) This will extract the elements from the page and return them in a dictionary. It can be tested from the command line like: $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 {'children': '1', 'hired': '3/6/1963', 'marital_status': 'Married'} One thing to note is that since we didn't define a single source attribute like we did in EmployeeList , we need to pass one on the command line with --source . This lets you quickly try your scraper against multiple variants of a page as needed.","title":"Scraping a Single Page"},{"location":"scraper-basics/#chaining-pages-together","text":"Most moderately complex sites will require chaining data together from multiple pages to get a complete object. Let's revisit EmployeeList and have it return instances of EmployeeDetail to tell spatula that more work is needed: class EmployeeList ( HtmlListPage ): # by providing this here, it can be omitted on the command line # useful in cases where the scraper is only meant for one page source = \"https://yoyodyne-propulsion.herokuapp.com/staff\" # each row represents an employee selector = CSS ( \"#employees tbody tr\" ) def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( dict ( first = first . text , last = last . text , position = position . text , ), source = XPath ( \"./a/@href\" ) . match_one ( details ), ) And we can revisit EmployeeDetail to tell it to combine the data it collects with the data passed in from the parent page: class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape, # in this case a dict we can expand here ** self . input , ) Now a run looks like: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: EmployeeDetail(input={'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} source=https://yoyodyne-propulsion.herokuapp.com/staff) 2: EmployeeDetail(input={'first': 'John', 'last': 'Bigboot\u00e9', 'position': 'Executive Vice President'} source=https:/yoyodyne-propulsion.herokuapp.com/staff/2) ... 10: EmployeeDetail(input={'first': 'John', 'last': 'Fish', 'position': 'Marine R&D'} source=https:/yoyodyne-propulsion.herokuapp.com/staff/20) By default, spatula test just shows the result of the page you're working on, but you can see that it is now returning page objects with the data and a source set.","title":"Chaining Pages Together"},{"location":"scraper-basics/#running-a-scrape","text":"Now that we're happy with our individual page scrapers, we can run the full scrape and write the data to disk. For this we use the spatula scrape command: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/2 ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/100 INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/101 success: wrote 10 objects to _scrapes/2021-06-03/001 And now our scraped data is on disk, ready for you to use! If you look at a data file you'll see that it has the full data for an individual: { \"marital_status\" : \"Single\" , \"children\" : \"0\" , \"hired\" : \"9/9/1963\" , \"first\" : \"John\" , \"last\" : \"Omar\" , \"position\" : \"Imports & Exports\" }","title":"Running a Scrape"},{"location":"scraper-basics/#using-spatula-within-other-scripts","text":"Perhaps you don't want to write your output to disk. If you want to post-process your data further or use it as part of a larger pipeline a page's do_scrape method lets you do just that. It returns a generator that you can use to process items as you see fit. For example: page = EmployeeList () for e in page . do_scrape (): print ( e ) You an do whatever you wish with these results, output them in a custom format, save them to your database, etc.","title":"Using spatula Within Other Scripts"},{"location":"scraper-basics/#pagination","text":"While writing the list page we ignored pagination, let's go ahead and add it now. If we override the get_next_source method on our EmployeeLList class, spatula will continue to the next page once it has called process_item on all elements on the current page. # add this within EmployeeList def get_next_source ( self ): try : return XPath ( \"//a[contains(text(), 'Next')]/@href\" ) . match_one ( self . root ) except SelectorError : pass You'll notice the output of spatula test quickstart.EmployeeList has now changed: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: {'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} ... paginating for EmployeeList source=https://yoyodyne-propulsion.herokuapp.com/staff?page=2 INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff?page=2 ... 45: EmployeeDetail(input={'first': 'John', 'last': 'Ya Ya', 'position': 'Computer Design Specialist'} source=https://yoyodyne-propulsion.herokuapp.com/staff/101)","title":"Pagination"},{"location":"scraper-basics/#error-handling","text":"Now that we're grabbing all 45 employees, kick off another full scrape: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/404 Traceback (most recent call last): ... scrapelib.HTTPError: 404 while retrieving https://yoyodyne-propulsion.herokuapp.com/staff/404 An error! It turns out that one of the employee pages isn't loading correctly. Sometimes it is best to let these errors propagate so you can try to fix the broken scraper. Other times it makes more sense to handle the error and move on. If you wish to do that, you can override process_error_response . Add the following to EmployeeDetail : def process_error_response ( self , exception ): # every Page subclass has a built-in logger object self . logger . warning ( exception ) Run the scrape again to see this in action: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff ... WARNING:quickstart.EmployeeDetail:404 while retrieving https://yoyodyne-propulsion.herokuapp.com/staff/404 ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/101 success: wrote 44 objects to _scrapes/2021-06-03/002","title":"Error Handling"}]}