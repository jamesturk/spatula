{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 spatula is a modern Python library for writing maintainable web scrapers. Source: https://github.com/jamesturk/spatula Documentation: https://jamesturk.github.io/spatula/ Issues: https://github.com/jamesturk/spatula/issues Features \u00b6 Page-oriented design : Encourages writing understandable & maintainable scrapers. Not Just HTML : Provides built in handlers for common data formats including CSV, JSON, XML, PDF, and Excel. Or write your own. Fast HTML parsing : Uses lxml.html for fast, consistent, and reliable parsing of HTML. Flexible Data Model Support : Compatible with dataclasses , attrs , pydantic , or bring your own data model classes for storing & validating your scraped data. CLI Tools : Offers several CLI utilities that can help streamline development & testing cycle. Fully Typed : Makes full use of Python 3 type annotations. Installation \u00b6 spatula is on PyPI , and can be installed via any standard package management tool: poetry add spatula or: pip install spatula Example \u00b6 An example of a fairly simple two-page scrape, read A First Scraper for a walkthrough of how it was built. from spatula import HtmlPage , HtmlListPage , CSS , XPath , SelectorError class EmployeeList ( HtmlListPage ): # by providing this here, it can be omitted on the command line # useful in cases where the scraper is only meant for one page source = \"https://yoyodyne-propulsion.herokuapp.com/staff\" # each row represents an employee selector = CSS ( \"#employees tbody tr\" ) def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( dict ( first = first . text , last = last . text , position = position . text , ), source = XPath ( \"./a/@href\" ) . match_one ( details ), ) def get_next_source ( self ): try : return XPath ( \"//a[contains(text(), 'Next')]/@href\" ) . match_one ( self . root ) except SelectorError : pass class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape ** self . input , ) def process_error_response ( self , exc ): self . logger . warning ( exc )","title":"Overview"},{"location":"#overview","text":"spatula is a modern Python library for writing maintainable web scrapers. Source: https://github.com/jamesturk/spatula Documentation: https://jamesturk.github.io/spatula/ Issues: https://github.com/jamesturk/spatula/issues","title":"Overview"},{"location":"#features","text":"Page-oriented design : Encourages writing understandable & maintainable scrapers. Not Just HTML : Provides built in handlers for common data formats including CSV, JSON, XML, PDF, and Excel. Or write your own. Fast HTML parsing : Uses lxml.html for fast, consistent, and reliable parsing of HTML. Flexible Data Model Support : Compatible with dataclasses , attrs , pydantic , or bring your own data model classes for storing & validating your scraped data. CLI Tools : Offers several CLI utilities that can help streamline development & testing cycle. Fully Typed : Makes full use of Python 3 type annotations.","title":"Features"},{"location":"#installation","text":"spatula is on PyPI , and can be installed via any standard package management tool: poetry add spatula or: pip install spatula","title":"Installation"},{"location":"#example","text":"An example of a fairly simple two-page scrape, read A First Scraper for a walkthrough of how it was built. from spatula import HtmlPage , HtmlListPage , CSS , XPath , SelectorError class EmployeeList ( HtmlListPage ): # by providing this here, it can be omitted on the command line # useful in cases where the scraper is only meant for one page source = \"https://yoyodyne-propulsion.herokuapp.com/staff\" # each row represents an employee selector = CSS ( \"#employees tbody tr\" ) def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( dict ( first = first . text , last = last . text , position = position . text , ), source = XPath ( \"./a/@href\" ) . match_one ( details ), ) def get_next_source ( self ): try : return XPath ( \"//a[contains(text(), 'Next')]/@href\" ) . match_one ( self . root ) except SelectorError : pass class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape ** self . input , ) def process_error_response ( self , exc ): self . logger . warning ( exc )","title":"Example"},{"location":"advanced-techniques/","text":"Advanced Techniques \u00b6 Specifying Dependencies \u00b6 While the pattern laid out in Scraper Basics is fairly common, sometimes data is laid out in other ways that doesn't fit as neatly with the list & detail page workflow. For example, take a look at the page https://yoyodyne-propulsion.herokuapp.com/awards , which lists some awards that some Yoyodyne employees have won. In some cases you may decide to scrape this data separately using the typical HtmlListPage approach, but let's say you want each employee to have a list of their awards as part of the EmployeeList / EmployeeDetail scrape. Scraping the New Page \u00b6 First let's write a quick page scraper to grab the award name & year and associate them with a person's name: # add to imports from collections import defaultdict from dataclasses import dataclass @dataclass class Award : award : str year : str class AwardsPage ( HtmlPage ): source = \"https://yoyodyne-propulsion.herokuapp.com/awards\" def process_page ( self ): # augmentation pages will almost always return some kind of dictionary mapping = defaultdict ( list ) award_cards = CSS ( \".card .large\" , num_items = 9 ) . match ( self . root ) for item in award_cards : award = CSS ( \"h2\" ) . match_one ( item ) . text . strip () year = CSS ( \"small\" ) . match_one ( item ) . text # make sure we got exactly 2 <dd> tags, and we take the second name = CSS ( \"dd\" ) . match ( item , num_items = 2 )[ 1 ] . text # map the data based on a key we know we have elsewhere in the scrape mapping [ name ] . append ( Award ( award = award , year = year )) return mapping Running this scrape yields a single dictionary: $ spatula test quickstart.AwardsPage INFO:quickstart.AwardsPage:fetching https://yoyodyne-propulsion.herokuapp.com/awards defaultdict(<class 'list'>, {'John Coyote': [Award(award='Album of the Year', year='1997')], 'John Fish': [Award(award='Cousteau Society Award', year='1989')], 'John Lee': [Award(award='Most Creative Loophole', year='1985')], 'John Many Jars': [Award(award='2nd Place, Most Jars Category', year='1987')], \"John O'Connor\": [Award(award='Nobel Prize in Physics', year='2986')], 'John Two Horns': [Award(award='Paralegal of the Year', year='1999')], 'John Whorfin': [Award(award='Nobel Prize in Physics', year='1934'), Award(award='Best Supporting Actor', year='1985')], 'John Ya Ya': [Award(award='ACM Award', year='1986')]}) Add the Dependency \u00b6 Now that this page is working, we can connect it to our previously written EmployeeList scrape. class EmployeeDetail ( HtmlPage ): dependencies = { \"award_mapping\" : AwardsPage ()} ... This line ensures that each instance of EmployeeDetail will be have a self.award_mapping attribute, pre-populated with the result of AwardsPage . If you pass an instance of a page then each EmployeeDetail will share a cached copy of AwardsPage , ensuring it is only scraped once. Use the Dependency Data \u00b6 The final step now is to actually use the mapping to attach the awards to the employees: class Employee ( PartialEmployee ): marital_status : str children : int hired : str awards : list [ Award ] And then within EmployeeDetail : def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return Employee ( first = self . input . first , last = self . input . last , position = self . input . position , url = self . input . url , marital_status = marital_status . text , children = children . text , hired = hired . text , awards = self . award_mapping [ f \" { self . input . first } { self . input . last } \" ], ) We can test by passing fake data with a person that has an award: $ spatula test quickstart.EmployeeDetail --data first = John --data last = Fish INFO:quickstart.AwardsPage:fetching https://yoyodyne-propulsion.herokuapp.com/awards INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/1 {'awards': [Award(award='Cousteau Society Award', year='1989')], 'children': '0', 'first': 'John', 'hired': '10/31/1938', 'last': 'Fish', 'marital_status': 'Single', 'position': 'Engineer', 'url': 'https://yoyodyne-propulsion.herokuapp.com/staff/1'} Note that before fetching the EmployeeDetail page, AwardsPage is fetched, and the awards data is then correctly attached to John Fish. Advanced Sources \u00b6 NullSource \u00b6 Every Page has a source which is fetched when it is executed. There are cases where you may wish to avoid that behavior. If you set NullSource for a page, no HTTP request will be performed prior to the process_item method being called. A common use for this is dispatching multiple detail pages without a corresponding list page. class NebraskaPageGenerator ( ListPage ): \"\"\" When scraping the Nebraska legislature, the pages are named http://news.legislature.ne.gov/dist01/ through http://news.legislature.ne.gov/dist49/ but with out an easy-to-scrape source. So we use this method to mimic the results that a ListPage would yield, without a wasted request. \"\"\" source = NullSource () def process_page ( self ): for n in range ( 1 , 50 ): yield NebraskaLegPage ( source = f \"http://news.legislature.ne.gov/dist { n : 02d } /\" ) Custom Sources \u00b6 Sometimes you need a page to do something that isn't easy to do with a single URL object. To derive a custom Source , simply override the get_response method in your own custom source class. For example: import scrapelib import requests from spatula import Source class FauxFormSource ( Source ): \"\"\" emulate a case where we need to get a hidden input value to successfully retrieve a form \"\"\" def get_response ( self , scraper : scrapelib . Scraper ) -> requests . models . Response : url = \"https://example.com/\" resp = scraper . get ( url ) root = lxml . html . fromstring ( resp . content ) token = form . xpath ( \".//input[@name='csrftoken']\" )[ 0 ] . value # do second request with data resp = scraper . post ( self . url , { \"csrftoken\" : token }) return resp You can do whatever you want within get_response as long as something resembling a requests.Response is returned. Custom Page Types \u00b6 Another powerful technique is to define your own Page or ListPage subclasses. Most of the existing page types are fairly simple, typically only overriding postprocess_response , which is called after any source is turned into self.response , but before process_item is called. If you wanted to use BeautifulSoup instead of spatula's default lxml.html you could define a custom SoupPage : from bs4 import BeautifulSoup from spatula import Page class SoupPage ( Page ): def postprocess_response ( self ) -> None : # self.response is guaranteed to have been set by now self . soup = BeautifulSoup ( self . response . content ) This would let any pages that derive from SoupPage use self.soup the same way that self.root is available on HtmlPage .","title":"Advanced Techniques"},{"location":"advanced-techniques/#advanced-techniques","text":"","title":"Advanced Techniques"},{"location":"advanced-techniques/#specifying-dependencies","text":"While the pattern laid out in Scraper Basics is fairly common, sometimes data is laid out in other ways that doesn't fit as neatly with the list & detail page workflow. For example, take a look at the page https://yoyodyne-propulsion.herokuapp.com/awards , which lists some awards that some Yoyodyne employees have won. In some cases you may decide to scrape this data separately using the typical HtmlListPage approach, but let's say you want each employee to have a list of their awards as part of the EmployeeList / EmployeeDetail scrape.","title":"Specifying Dependencies"},{"location":"advanced-techniques/#scraping-the-new-page","text":"First let's write a quick page scraper to grab the award name & year and associate them with a person's name: # add to imports from collections import defaultdict from dataclasses import dataclass @dataclass class Award : award : str year : str class AwardsPage ( HtmlPage ): source = \"https://yoyodyne-propulsion.herokuapp.com/awards\" def process_page ( self ): # augmentation pages will almost always return some kind of dictionary mapping = defaultdict ( list ) award_cards = CSS ( \".card .large\" , num_items = 9 ) . match ( self . root ) for item in award_cards : award = CSS ( \"h2\" ) . match_one ( item ) . text . strip () year = CSS ( \"small\" ) . match_one ( item ) . text # make sure we got exactly 2 <dd> tags, and we take the second name = CSS ( \"dd\" ) . match ( item , num_items = 2 )[ 1 ] . text # map the data based on a key we know we have elsewhere in the scrape mapping [ name ] . append ( Award ( award = award , year = year )) return mapping Running this scrape yields a single dictionary: $ spatula test quickstart.AwardsPage INFO:quickstart.AwardsPage:fetching https://yoyodyne-propulsion.herokuapp.com/awards defaultdict(<class 'list'>, {'John Coyote': [Award(award='Album of the Year', year='1997')], 'John Fish': [Award(award='Cousteau Society Award', year='1989')], 'John Lee': [Award(award='Most Creative Loophole', year='1985')], 'John Many Jars': [Award(award='2nd Place, Most Jars Category', year='1987')], \"John O'Connor\": [Award(award='Nobel Prize in Physics', year='2986')], 'John Two Horns': [Award(award='Paralegal of the Year', year='1999')], 'John Whorfin': [Award(award='Nobel Prize in Physics', year='1934'), Award(award='Best Supporting Actor', year='1985')], 'John Ya Ya': [Award(award='ACM Award', year='1986')]})","title":"Scraping the New Page"},{"location":"advanced-techniques/#add-the-dependency","text":"Now that this page is working, we can connect it to our previously written EmployeeList scrape. class EmployeeDetail ( HtmlPage ): dependencies = { \"award_mapping\" : AwardsPage ()} ... This line ensures that each instance of EmployeeDetail will be have a self.award_mapping attribute, pre-populated with the result of AwardsPage . If you pass an instance of a page then each EmployeeDetail will share a cached copy of AwardsPage , ensuring it is only scraped once.","title":"Add the Dependency"},{"location":"advanced-techniques/#use-the-dependency-data","text":"The final step now is to actually use the mapping to attach the awards to the employees: class Employee ( PartialEmployee ): marital_status : str children : int hired : str awards : list [ Award ] And then within EmployeeDetail : def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return Employee ( first = self . input . first , last = self . input . last , position = self . input . position , url = self . input . url , marital_status = marital_status . text , children = children . text , hired = hired . text , awards = self . award_mapping [ f \" { self . input . first } { self . input . last } \" ], ) We can test by passing fake data with a person that has an award: $ spatula test quickstart.EmployeeDetail --data first = John --data last = Fish INFO:quickstart.AwardsPage:fetching https://yoyodyne-propulsion.herokuapp.com/awards INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/1 {'awards': [Award(award='Cousteau Society Award', year='1989')], 'children': '0', 'first': 'John', 'hired': '10/31/1938', 'last': 'Fish', 'marital_status': 'Single', 'position': 'Engineer', 'url': 'https://yoyodyne-propulsion.herokuapp.com/staff/1'} Note that before fetching the EmployeeDetail page, AwardsPage is fetched, and the awards data is then correctly attached to John Fish.","title":"Use the Dependency Data"},{"location":"advanced-techniques/#advanced-sources","text":"","title":"Advanced Sources"},{"location":"advanced-techniques/#nullsource","text":"Every Page has a source which is fetched when it is executed. There are cases where you may wish to avoid that behavior. If you set NullSource for a page, no HTTP request will be performed prior to the process_item method being called. A common use for this is dispatching multiple detail pages without a corresponding list page. class NebraskaPageGenerator ( ListPage ): \"\"\" When scraping the Nebraska legislature, the pages are named http://news.legislature.ne.gov/dist01/ through http://news.legislature.ne.gov/dist49/ but with out an easy-to-scrape source. So we use this method to mimic the results that a ListPage would yield, without a wasted request. \"\"\" source = NullSource () def process_page ( self ): for n in range ( 1 , 50 ): yield NebraskaLegPage ( source = f \"http://news.legislature.ne.gov/dist { n : 02d } /\" )","title":"NullSource"},{"location":"advanced-techniques/#custom-sources","text":"Sometimes you need a page to do something that isn't easy to do with a single URL object. To derive a custom Source , simply override the get_response method in your own custom source class. For example: import scrapelib import requests from spatula import Source class FauxFormSource ( Source ): \"\"\" emulate a case where we need to get a hidden input value to successfully retrieve a form \"\"\" def get_response ( self , scraper : scrapelib . Scraper ) -> requests . models . Response : url = \"https://example.com/\" resp = scraper . get ( url ) root = lxml . html . fromstring ( resp . content ) token = form . xpath ( \".//input[@name='csrftoken']\" )[ 0 ] . value # do second request with data resp = scraper . post ( self . url , { \"csrftoken\" : token }) return resp You can do whatever you want within get_response as long as something resembling a requests.Response is returned.","title":"Custom Sources"},{"location":"advanced-techniques/#custom-page-types","text":"Another powerful technique is to define your own Page or ListPage subclasses. Most of the existing page types are fairly simple, typically only overriding postprocess_response , which is called after any source is turned into self.response , but before process_item is called. If you wanted to use BeautifulSoup instead of spatula's default lxml.html you could define a custom SoupPage : from bs4 import BeautifulSoup from spatula import Page class SoupPage ( Page ): def postprocess_response ( self ) -> None : # self.response is guaranteed to have been set by now self . soup = BeautifulSoup ( self . response . content ) This would let any pages that derive from SoupPage use self.soup the same way that self.root is available on HtmlPage .","title":"Custom Page Types"},{"location":"anatomy-of-a-scrape/","text":"Anatomy of a Scrape \u00b6 This diagram illustrates the control flow when Page.do_scrape is invoked programmatically or via spatula scrape : Dependencies \u00b6 When a scrape is initiated, the first thing spatula will do is examine any dependencies declared on the page being invoked. Each dependency is resolved (in essence a full scrape of its own) with the resulting data saved to an internal cache (to avoid duplicating scrapes of shared dependencies). See Specifying Dependencies for a practical application. Ensuring a Source Exists \u00b6 Once any dependencies are resolved, the page attempts to resolve a source attribute. There are a number of places that a source might come from, in order of precedence: overridden using the --source parameter if using a CLI scrape passed in via class constructor (e.g. if this is a subpage) set as a class attribute ( Page.source ) retrieved via get_source_from_input Note If any of these methods return a string, it will be automatically converted to a URL . Processing the Response \u00b6 Once a source is obtained, source.get_response is called, which typically means an HTTP request is performed. If an exception is raised, it will be passed to process_error_response . If all goes well, the response attribute of the page class is set and postprocess_response will be called. This is where classes like HtmlPage and CsvListPage process the response and do any additional parsing required. User Code: Processing Page Contents \u00b6 Once the response has been processed, spatula will call the page's process_page method. In a standard use of spatula this is the first time that user-written code is run. process_page can return or yield actual data, or additional pages to continue the scrape. Handling Subpages \u00b6 If subpages are returned, each of them will be handled in essentially the same cycle. Pagination \u00b6 After process_page terminates, spatula checks if there is a result from get_next_source . If so, a new instance of the page class is instantiated with the new source set & the process is repeated from processing the response .","title":"Anatomy of a Scrape"},{"location":"anatomy-of-a-scrape/#anatomy-of-a-scrape","text":"This diagram illustrates the control flow when Page.do_scrape is invoked programmatically or via spatula scrape :","title":"Anatomy of a Scrape"},{"location":"anatomy-of-a-scrape/#dependencies","text":"When a scrape is initiated, the first thing spatula will do is examine any dependencies declared on the page being invoked. Each dependency is resolved (in essence a full scrape of its own) with the resulting data saved to an internal cache (to avoid duplicating scrapes of shared dependencies). See Specifying Dependencies for a practical application.","title":"Dependencies"},{"location":"anatomy-of-a-scrape/#ensuring-a-source-exists","text":"Once any dependencies are resolved, the page attempts to resolve a source attribute. There are a number of places that a source might come from, in order of precedence: overridden using the --source parameter if using a CLI scrape passed in via class constructor (e.g. if this is a subpage) set as a class attribute ( Page.source ) retrieved via get_source_from_input Note If any of these methods return a string, it will be automatically converted to a URL .","title":"Ensuring a Source Exists"},{"location":"anatomy-of-a-scrape/#processing-the-response","text":"Once a source is obtained, source.get_response is called, which typically means an HTTP request is performed. If an exception is raised, it will be passed to process_error_response . If all goes well, the response attribute of the page class is set and postprocess_response will be called. This is where classes like HtmlPage and CsvListPage process the response and do any additional parsing required.","title":"Processing the Response"},{"location":"anatomy-of-a-scrape/#user-code-processing-page-contents","text":"Once the response has been processed, spatula will call the page's process_page method. In a standard use of spatula this is the first time that user-written code is run. process_page can return or yield actual data, or additional pages to continue the scrape.","title":"User Code: Processing Page Contents"},{"location":"anatomy-of-a-scrape/#handling-subpages","text":"If subpages are returned, each of them will be handled in essentially the same cycle.","title":"Handling Subpages"},{"location":"anatomy-of-a-scrape/#pagination","text":"After process_page terminates, spatula checks if there is a result from get_next_source . If so, a new instance of the page class is instantiated with the new source set & the process is repeated from processing the response .","title":"Pagination"},{"location":"changelog/","text":"Changelog \u00b6 Note spatula 1.0 should be ready by Fall of 2021, providing a more stable interface to build upon, until then interfaces may change between releases. 0.8.2 - 2021-06-22 \u00b6 fix spatula --version to report correct version allow --data command line flags to override example_input values add caching of dependencies fix pagination on non-list pages add advanced documentation & anatomy of a scrape 0.8.1 - 2021-06-17 \u00b6 remove undocumented page_to_items function added Page.do_scrape to programmatically get all items from a scrape added --source parameter to scout & scrape commands 0.8.0 - 2021-06-15 \u00b6 remove undocumented Workflow allow using Page instances (as opposed to just the type) for scout & scrape add check for get_filename on output classes to override default filename improved automatic pydantic support add --timeout, --no-verify, --retries, --retry-wait options add --fastmode option to use local cache fix all CLI commands to obey various scraper options 0.7.1 - 2021-06-14 \u00b6 remove undocumented default behavior for get_source_from_input major documentation overhaul fixes for scout scrape when working with raw data returns 0.7.0 - 2021-06-04 \u00b6 add spatula scout command make error messages a bit more clear improvements to documentation added more CLI options to control verbosity, user agent, etc. if module cannot be found, search current directory 0.6.0 - 2021-04-12 \u00b6 add full typing to library small bugfixes 0.5.0 - 2021-02-04 \u00b6 add ExcelListPage improve Page.logger and CLI output move to simpler Workflow class spatula scrape can now take the name of a page, will use default Workflow bugfix: inconsistent name for process_error_response 0.4.1 - 2021-02-01 \u00b6 bugfix: dependencies are instantiated from parent page input 0.4.0 - 2021-02-01 \u00b6 restore Python 3.7 compatibility add behavior to handle returning additional Page subclasses to continue scraping add default behavior when Page.input has a url attribute. add PdfPage add page_to_items helper add Page.example_input and Page.example_source for test command add Page.logger for logging allow use of dataclasses in addition to attrs as input objects improve output of HTML elements bugfix: not specifying a page processor on workflow is no longer an error 0.3.0 - 2021-01-18 \u00b6 first documented major release","title":"Changelog"},{"location":"changelog/#changelog","text":"Note spatula 1.0 should be ready by Fall of 2021, providing a more stable interface to build upon, until then interfaces may change between releases.","title":"Changelog"},{"location":"changelog/#082-2021-06-22","text":"fix spatula --version to report correct version allow --data command line flags to override example_input values add caching of dependencies fix pagination on non-list pages add advanced documentation & anatomy of a scrape","title":"0.8.2 - 2021-06-22"},{"location":"changelog/#081-2021-06-17","text":"remove undocumented page_to_items function added Page.do_scrape to programmatically get all items from a scrape added --source parameter to scout & scrape commands","title":"0.8.1 - 2021-06-17"},{"location":"changelog/#080-2021-06-15","text":"remove undocumented Workflow allow using Page instances (as opposed to just the type) for scout & scrape add check for get_filename on output classes to override default filename improved automatic pydantic support add --timeout, --no-verify, --retries, --retry-wait options add --fastmode option to use local cache fix all CLI commands to obey various scraper options","title":"0.8.0 - 2021-06-15"},{"location":"changelog/#071-2021-06-14","text":"remove undocumented default behavior for get_source_from_input major documentation overhaul fixes for scout scrape when working with raw data returns","title":"0.7.1 - 2021-06-14"},{"location":"changelog/#070-2021-06-04","text":"add spatula scout command make error messages a bit more clear improvements to documentation added more CLI options to control verbosity, user agent, etc. if module cannot be found, search current directory","title":"0.7.0 - 2021-06-04"},{"location":"changelog/#060-2021-04-12","text":"add full typing to library small bugfixes","title":"0.6.0 - 2021-04-12"},{"location":"changelog/#050-2021-02-04","text":"add ExcelListPage improve Page.logger and CLI output move to simpler Workflow class spatula scrape can now take the name of a page, will use default Workflow bugfix: inconsistent name for process_error_response","title":"0.5.0 - 2021-02-04"},{"location":"changelog/#041-2021-02-01","text":"bugfix: dependencies are instantiated from parent page input","title":"0.4.1 - 2021-02-01"},{"location":"changelog/#040-2021-02-01","text":"restore Python 3.7 compatibility add behavior to handle returning additional Page subclasses to continue scraping add default behavior when Page.input has a url attribute. add PdfPage add page_to_items helper add Page.example_input and Page.example_source for test command add Page.logger for logging allow use of dataclasses in addition to attrs as input objects improve output of HTML elements bugfix: not specifying a page processor on workflow is no longer an error","title":"0.4.0 - 2021-02-01"},{"location":"changelog/#030-2021-01-18","text":"first documented major release","title":"0.3.0 - 2021-01-18"},{"location":"cli/","text":"Command Line Interface \u00b6 spatula provides a command line interface that is useful for iterative development of scrapers. Once installed within your Python environment, spatula can be invoked on the command line. E.g.: (scrape-venv) ~/scrape-proj $ spatula --version spatula, version 0.8.2 Or with poetry: ~/scrape-proj $ poetry run spatula --version spatula, version 0.8.2 The CLI provides four useful subcommands for different stages of development: spatula \u00b6 Usage: spatula [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --version boolean Show the version and exit. False --help boolean Show this message and exit. False scout \u00b6 Run first step of scrape & output data to a JSON file. This command is intended to be used to detect at a first approximation whether or not a full scrape might need to be run. If the first layer detects any changes it is safe to say that the full run will as well. This will work in the common case where a new subpage is added or removed. Of course in more advanced cases this depends upon the first page being scraped (typically a ListPage derivative) surfacing enough information (perhaps a last_updated date) to know whether any of the other pages have been scraped. Usage: spatula scout [OPTIONS] INITIAL_PAGE_NAME Options: Name Type Description Default -s , --source text Provide (or override) source URL required -o , --output-file text override default output file [default: scout.json]. scout.json -ua , --user-agent text override default user-agent spatula 0.8.2 --rpm integer set requests per minute (default: 60) 60 --timeout integer set HTTP request timeout in seconds (default: 5) 5 --verify / --no-verify boolean control verification of SSL certs True --retries integer configure how many retries to perform on HTTP request error (default: 0) 0 --retry-wait integer configure how many seconds to wait on HTTP request error (default: 10) 10 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --fastmode boolean use a cache to avoid making unnecessary requests False --help boolean Show this message and exit. False scrape \u00b6 Run full scrape, and output data to disk. Usage: spatula scrape [OPTIONS] INITIAL_PAGE_NAME Options: Name Type Description Default -o , --output-dir text override default output directory. required -s , --source text Provide (or override) source URL required -ua , --user-agent text override default user-agent spatula 0.8.2 --rpm integer set requests per minute (default: 60) 60 --timeout integer set HTTP request timeout in seconds (default: 5) 5 --verify / --no-verify boolean control verification of SSL certs True --retries integer configure how many retries to perform on HTTP request error (default: 0) 0 --retry-wait integer configure how many seconds to wait on HTTP request error (default: 10) 10 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --fastmode boolean use a cache to avoid making unnecessary requests False --help boolean Show this message and exit. False shell \u00b6 Start a session to interact with a particular page. Usage: spatula shell [OPTIONS] URL Options: Name Type Description Default -X , --verb text set HTTP verb such as POST GET -ua , --user-agent text override default user-agent spatula 0.8.2 --rpm integer set requests per minute (default: 60) 60 --timeout integer set HTTP request timeout in seconds (default: 5) 5 --verify / --no-verify boolean control verification of SSL certs True --retries integer configure how many retries to perform on HTTP request error (default: 0) 0 --retry-wait integer configure how many seconds to wait on HTTP request error (default: 10) 10 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --fastmode boolean use a cache to avoid making unnecessary requests False --help boolean Show this message and exit. False test \u00b6 Scrape a single page and see output immediately. This eases the common cycle of making modifications to a scraper, running a scrape (possibly with long-running but irrelevant portions commented out), and comparing output to what is expected. test can also be useful for debugging existing scrapers, you can see exactly what a single step of the scrape is providing, to help narrow down where erroneous data is coming from. Example: $ spatula test path.to.ClassName --source https://example.com This will run the scraper defined at path.to.ClassName against the provided URL. Usage: spatula test [OPTIONS] CLASS_NAME Options: Name Type Description Default --interactive / --no-interactive boolean Interactively prompt for missing data. False -d , --data text Provide input data in name=value pairs. required -s , --source text Provide (or override) source URL required --pagination / --no-pagination boolean Determine whether or not pagination should be followed or one page is enough for testing True -ua , --user-agent text override default user-agent spatula 0.8.2 --rpm integer set requests per minute (default: 60) 60 --timeout integer set HTTP request timeout in seconds (default: 5) 5 --verify / --no-verify boolean control verification of SSL certs True --retries integer configure how many retries to perform on HTTP request error (default: 0) 0 --retry-wait integer configure how many seconds to wait on HTTP request error (default: 10) 10 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --fastmode boolean use a cache to avoid making unnecessary requests False --help boolean Show this message and exit. False","title":"Command Line Interface"},{"location":"cli/#command-line-interface","text":"spatula provides a command line interface that is useful for iterative development of scrapers. Once installed within your Python environment, spatula can be invoked on the command line. E.g.: (scrape-venv) ~/scrape-proj $ spatula --version spatula, version 0.8.2 Or with poetry: ~/scrape-proj $ poetry run spatula --version spatula, version 0.8.2 The CLI provides four useful subcommands for different stages of development:","title":"Command Line Interface"},{"location":"cli/#spatula","text":"Usage: spatula [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --version boolean Show the version and exit. False --help boolean Show this message and exit. False","title":"spatula"},{"location":"cli/#scout","text":"Run first step of scrape & output data to a JSON file. This command is intended to be used to detect at a first approximation whether or not a full scrape might need to be run. If the first layer detects any changes it is safe to say that the full run will as well. This will work in the common case where a new subpage is added or removed. Of course in more advanced cases this depends upon the first page being scraped (typically a ListPage derivative) surfacing enough information (perhaps a last_updated date) to know whether any of the other pages have been scraped. Usage: spatula scout [OPTIONS] INITIAL_PAGE_NAME Options: Name Type Description Default -s , --source text Provide (or override) source URL required -o , --output-file text override default output file [default: scout.json]. scout.json -ua , --user-agent text override default user-agent spatula 0.8.2 --rpm integer set requests per minute (default: 60) 60 --timeout integer set HTTP request timeout in seconds (default: 5) 5 --verify / --no-verify boolean control verification of SSL certs True --retries integer configure how many retries to perform on HTTP request error (default: 0) 0 --retry-wait integer configure how many seconds to wait on HTTP request error (default: 10) 10 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --fastmode boolean use a cache to avoid making unnecessary requests False --help boolean Show this message and exit. False","title":"scout"},{"location":"cli/#scrape","text":"Run full scrape, and output data to disk. Usage: spatula scrape [OPTIONS] INITIAL_PAGE_NAME Options: Name Type Description Default -o , --output-dir text override default output directory. required -s , --source text Provide (or override) source URL required -ua , --user-agent text override default user-agent spatula 0.8.2 --rpm integer set requests per minute (default: 60) 60 --timeout integer set HTTP request timeout in seconds (default: 5) 5 --verify / --no-verify boolean control verification of SSL certs True --retries integer configure how many retries to perform on HTTP request error (default: 0) 0 --retry-wait integer configure how many seconds to wait on HTTP request error (default: 10) 10 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --fastmode boolean use a cache to avoid making unnecessary requests False --help boolean Show this message and exit. False","title":"scrape"},{"location":"cli/#shell","text":"Start a session to interact with a particular page. Usage: spatula shell [OPTIONS] URL Options: Name Type Description Default -X , --verb text set HTTP verb such as POST GET -ua , --user-agent text override default user-agent spatula 0.8.2 --rpm integer set requests per minute (default: 60) 60 --timeout integer set HTTP request timeout in seconds (default: 5) 5 --verify / --no-verify boolean control verification of SSL certs True --retries integer configure how many retries to perform on HTTP request error (default: 0) 0 --retry-wait integer configure how many seconds to wait on HTTP request error (default: 10) 10 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --fastmode boolean use a cache to avoid making unnecessary requests False --help boolean Show this message and exit. False","title":"shell"},{"location":"cli/#test","text":"Scrape a single page and see output immediately. This eases the common cycle of making modifications to a scraper, running a scrape (possibly with long-running but irrelevant portions commented out), and comparing output to what is expected. test can also be useful for debugging existing scrapers, you can see exactly what a single step of the scrape is providing, to help narrow down where erroneous data is coming from. Example: $ spatula test path.to.ClassName --source https://example.com This will run the scraper defined at path.to.ClassName against the provided URL. Usage: spatula test [OPTIONS] CLASS_NAME Options: Name Type Description Default --interactive / --no-interactive boolean Interactively prompt for missing data. False -d , --data text Provide input data in name=value pairs. required -s , --source text Provide (or override) source URL required --pagination / --no-pagination boolean Determine whether or not pagination should be followed or one page is enough for testing True -ua , --user-agent text override default user-agent spatula 0.8.2 --rpm integer set requests per minute (default: 60) 60 --timeout integer set HTTP request timeout in seconds (default: 5) 5 --verify / --no-verify boolean control verification of SSL certs True --retries integer configure how many retries to perform on HTTP request error (default: 0) 0 --retry-wait integer configure how many seconds to wait on HTTP request error (default: 10) 10 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --fastmode boolean use a cache to avoid making unnecessary requests False --help boolean Show this message and exit. False","title":"test"},{"location":"code_of_conduct/","text":"Code of Conduct \u00b6 Our Pledge \u00b6 We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards \u00b6 Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities \u00b6 Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope \u00b6 This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement: James Turk: dev@jamesturk.net All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines \u00b6 Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction \u00b6 Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning \u00b6 Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban \u00b6 Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban \u00b6 Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution \u00b6 This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Code of Conduct"},{"location":"code_of_conduct/#code-of-conduct","text":"","title":"Code of Conduct"},{"location":"code_of_conduct/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"code_of_conduct/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"code_of_conduct/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"code_of_conduct/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"code_of_conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement: James Turk: dev@jamesturk.net All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"code_of_conduct/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"code_of_conduct/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"code_of_conduct/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"code_of_conduct/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"code_of_conduct/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"code_of_conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"contributing/","text":"Contributing \u00b6 Issues \u00b6 Bug reports, questions, or feature requests can be submitted as GitHub issues . Developing Locally \u00b6 Before starting, you'll need poetry installed. pre-commit is also recommended. Fork spatula and check out your fork: $ git clone git@github.com:<your username>/spatula.git Install pre-commit hooks $ pre-commit install This will make sure that the linters run before each commit, saving you time. Install spatula and its development dependencies locally: $ cd spatula $ poetry install From here, you can use poetry run inv to run several useful maintenance commands. Running Tests \u00b6 poetry run inv test will run all tests and write coverage information to htmlcov/index.html Linting & Type Checking \u00b6 poetry run inv lint will run flake8 and black to lint the code style. poetry run inv mypy will run the mypy type checker. Building Docs \u00b6 poetry run inv docs will build the docs and watch for changes.","title":"Contributing"},{"location":"contributing/#contributing","text":"","title":"Contributing"},{"location":"contributing/#issues","text":"Bug reports, questions, or feature requests can be submitted as GitHub issues .","title":"Issues"},{"location":"contributing/#developing-locally","text":"Before starting, you'll need poetry installed. pre-commit is also recommended. Fork spatula and check out your fork: $ git clone git@github.com:<your username>/spatula.git Install pre-commit hooks $ pre-commit install This will make sure that the linters run before each commit, saving you time. Install spatula and its development dependencies locally: $ cd spatula $ poetry install From here, you can use poetry run inv to run several useful maintenance commands.","title":"Developing Locally"},{"location":"contributing/#running-tests","text":"poetry run inv test will run all tests and write coverage information to htmlcov/index.html","title":"Running Tests"},{"location":"contributing/#linting-type-checking","text":"poetry run inv lint will run flake8 and black to lint the code style. poetry run inv mypy will run the mypy type checker.","title":"Linting &amp; Type Checking"},{"location":"contributing/#building-docs","text":"poetry run inv docs will build the docs and watch for changes.","title":"Building Docs"},{"location":"data-models/","text":"Data Models \u00b6 Why Use Data Models? \u00b6 Back in Chaining Pages Together we saw that when chaining pages we can pass data through from the parent page. class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape, # in this case a dict we can expand here ** self . input , ) Dictionaries seem to work well for this since we can decide what data we want to grab on each page and combine them easily, but as scrapers tend to evolve over time it can be nice to have something a bit more self-documenting, and add the possibility to validate the data we're collecting. That's where we can introduce dataclasses , attrs , or pydantic models: dataclasses from dataclasses import dataclass @dataclass class Employee : first : str last : str position : str marital_status : str children : int hired : str attrs import attr @attr . s ( auto_attribs = True ) class Employee : first : str last : str position : str marital_status : str children : int hired : str pydantic from pydantic import BaseModel class Employee ( BaseModel ): first : str last : str position : str marital_status : str children : int hired : str Aren't sure which one to pick? dataclasses are built in to Python and easy to start with. You'll notice the examples barely differ, so it is easy to switch between them later on. If you want to add validation, pydantic is a great choice. And then we'll update EmployeeDetail.process_page to return our new Employee class: def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return Employee ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape, # in this case a dict we can expand here ** self . input , ) Let's give this a run: $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 Traceback (most recent call last): ... TypeError: __init__() missing 3 required positional arguments: 'first', 'last', and 'position' Of course! We're expecting self.input to contain these values, but when we're running spatula test it doesn't know what data would have been passed in. Defining input_type \u00b6 spatula provides a way to make the dependency on self.input more explicit, and restore our ability to test as a bonus side effect. Let's add a new data model that just includes the fields we're getting from the EmployeeList page: dataclasses @dataclass class PartialEmployee : first : str last : str position : str attrs @attr . s ( auto_attribs = True ) class PartialEmployee : first : str last : str position : str pydantic class PartialEmployee ( BaseModel ): first : str last : str position : str And then we'll update PersonDetail to set an input_type and stop assuming self.input is a dict : class EmployeeDetail ( HtmlPage ): input_type = PartialEmployee def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return Employee ( first = self . input . first , last = self . input . last , position = self . input . position , marital_status = marital_status . text , children = children . text , hired = hired . text , ) And now when we re-run the test command: $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" EmployeeDetail expects input (PartialEmployee): first: ~first last: ~last position: ~position INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 Employee(first='~first', last='~last', position='~position', marital_status='Married', children='1', hired='3/6/1963') Test data has been used, so even though EmployeeList didn't pass data into EmployeeDetail we can still see roughly what the data would look like if it had. Using Inheritance \u00b6 The above pattern is pretty useful & common. Often part of the data comes from one page, and the rest from another (or perhaps even more). A nice way to handle this without introducing a ton of redundancy is by setting up your models to inherit from one another: dataclasses from dataclasses import dataclass @dataclass class PartialEmployee : first : str last : str position : str @dataclass class Employee ( PartialEmployee ): marital_status : str children : int hired : str attrs import attr @attr . s ( auto_attribs = True ) class Employee : first : str last : str position : str @attr . s ( auto_attribs = True ) class PartialEmployee ( Employee ): marital_status : str children : int hired : str pydantic from pydantic import BaseModel class PartialEmployee ( BaseModel ): first : str last : str position : str class Employee ( PartialEmployee ): marital_status : str children : int hired : str Warning Be sure to remember to decorate the derived class(es) if using dataclasses or attrs . Fixing EmployeeList \u00b6 Don't forget to have EmployeeList return a PartialEmployee instance now instead of a dict : def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( PartialEmployee ( first = first . text , last = last . text , position = position . text , ), source = XPath ( \"./a/@href\" ) . match_one ( details ), ) Overriding Default Values \u00b6 Sometimes you may want to override default values (especially useful if the behavior of the second scrape varies on data from the first). via Command Line \u00b6 The --data flag to spatula test allows overriding input values with key=value pairs. $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" --data first = John --data last = Neptune EmployeeDetail expects input (PartialEmployee): first: John last: Neptune position: ~position INFO:ex03_data.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 Employee(first='John', last='Neptune', position='~position', marital_status='Married', children='1', hired='3/6/1963') Alternately, --interactive will prompt for input data. via example_input \u00b6 You can also provide the example_input attribute on the class in question. This value is assumed to be of type example_type . For example: class EmployeeDetail ( HtmlPage ): input_type = PartialEmployee example_input = PartialEmployee ( \"John\" , \"Neptune\" , \"Engineer\" ) example_source \u00b6 Like the above example_input you can define example_source to set a default value for the --source parameter when invoking spatula test class EmployeeDetail ( HtmlPage ): input_type = PartialEmployee example_input = PartialEmployee ( \"John\" , \"Neptune\" , \"Engineer\" ) example_source = \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" Warning Be sure not to confuse source with example_source . The former is used whenever the class is invoked without a source parameter, while example_source is only used when running spatula test . get_source_from_input \u00b6 It is not uncommon to want to capture a URL as part of the data and then use that URL as the next source . Let's go ahead and modify PartialEmployee to collect a URL: dataclasses @dataclass class PartialEmployee : first : str last : str position : str url : str attrs @attr . s ( auto_attribs = True ) class PartialEmployee : first : str last : str position : str url : str pydantic class PartialEmployee ( BaseModel ): first : str last : str position : str url : str And then we'll modify EmployeeList.process_item to capture this URL, and stop providing a redundant source : def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( PartialEmployee ( first = first . text , last = last . text , position = position . text , url = XPath ( \"./a/@href\" ) . match_one ( details ), ), ) And finally, add a get_source_from_input method to EmployeeDetail (as well as updating the other uses of Employee to have URL): class EmployeeDetail ( HtmlPage ): input_type = PartialEmployee example_input = PartialEmployee ( \"John\" , \"Neptune\" , \"Engineer\" , \"https://yoyodyne-propulsion.herokuapp.com/staff/1\" , ) def get_source_from_input ( self ): return self . input . url def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return Employee ( first = self . input . first , last = self . input . last , position = self . input . position , url = self . input . url , marital_status = marital_status . text , children = children . text , hired = hired . text , ) Of course, if you have a more complex situation you can do whatever you like in get_source_from_input . Data Models As Output \u00b6 When running spatula scrape data is written to disk as JSON. The exact method of obtaining that JSON varies a bit depending on what type of output you have: Raw dict : Output will match exactly. dataclasses : dataclass.asdict will be used. attrs : attr.asdict will be used to obtain a serializable representation. pydantic : the model's dict() method will be used. By default the filename will be a UUID, but if you wish to provide your own filename you can add a get_filename method to your model. Warning When providing get_filename be sure that your filenames are still unique (you may wish to still incorporate a UUID if you don't have a key you're sure is unique). spatula does not check for this, so you may overwrite data if your get_filename function does not guarantee uniqueness.","title":"Data Models"},{"location":"data-models/#data-models","text":"","title":"Data Models"},{"location":"data-models/#why-use-data-models","text":"Back in Chaining Pages Together we saw that when chaining pages we can pass data through from the parent page. class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape, # in this case a dict we can expand here ** self . input , ) Dictionaries seem to work well for this since we can decide what data we want to grab on each page and combine them easily, but as scrapers tend to evolve over time it can be nice to have something a bit more self-documenting, and add the possibility to validate the data we're collecting. That's where we can introduce dataclasses , attrs , or pydantic models: dataclasses from dataclasses import dataclass @dataclass class Employee : first : str last : str position : str marital_status : str children : int hired : str attrs import attr @attr . s ( auto_attribs = True ) class Employee : first : str last : str position : str marital_status : str children : int hired : str pydantic from pydantic import BaseModel class Employee ( BaseModel ): first : str last : str position : str marital_status : str children : int hired : str Aren't sure which one to pick? dataclasses are built in to Python and easy to start with. You'll notice the examples barely differ, so it is easy to switch between them later on. If you want to add validation, pydantic is a great choice. And then we'll update EmployeeDetail.process_page to return our new Employee class: def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return Employee ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape, # in this case a dict we can expand here ** self . input , ) Let's give this a run: $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 Traceback (most recent call last): ... TypeError: __init__() missing 3 required positional arguments: 'first', 'last', and 'position' Of course! We're expecting self.input to contain these values, but when we're running spatula test it doesn't know what data would have been passed in.","title":"Why Use Data Models?"},{"location":"data-models/#defining-input_type","text":"spatula provides a way to make the dependency on self.input more explicit, and restore our ability to test as a bonus side effect. Let's add a new data model that just includes the fields we're getting from the EmployeeList page: dataclasses @dataclass class PartialEmployee : first : str last : str position : str attrs @attr . s ( auto_attribs = True ) class PartialEmployee : first : str last : str position : str pydantic class PartialEmployee ( BaseModel ): first : str last : str position : str And then we'll update PersonDetail to set an input_type and stop assuming self.input is a dict : class EmployeeDetail ( HtmlPage ): input_type = PartialEmployee def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return Employee ( first = self . input . first , last = self . input . last , position = self . input . position , marital_status = marital_status . text , children = children . text , hired = hired . text , ) And now when we re-run the test command: $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" EmployeeDetail expects input (PartialEmployee): first: ~first last: ~last position: ~position INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 Employee(first='~first', last='~last', position='~position', marital_status='Married', children='1', hired='3/6/1963') Test data has been used, so even though EmployeeList didn't pass data into EmployeeDetail we can still see roughly what the data would look like if it had.","title":"Defining input_type"},{"location":"data-models/#using-inheritance","text":"The above pattern is pretty useful & common. Often part of the data comes from one page, and the rest from another (or perhaps even more). A nice way to handle this without introducing a ton of redundancy is by setting up your models to inherit from one another: dataclasses from dataclasses import dataclass @dataclass class PartialEmployee : first : str last : str position : str @dataclass class Employee ( PartialEmployee ): marital_status : str children : int hired : str attrs import attr @attr . s ( auto_attribs = True ) class Employee : first : str last : str position : str @attr . s ( auto_attribs = True ) class PartialEmployee ( Employee ): marital_status : str children : int hired : str pydantic from pydantic import BaseModel class PartialEmployee ( BaseModel ): first : str last : str position : str class Employee ( PartialEmployee ): marital_status : str children : int hired : str Warning Be sure to remember to decorate the derived class(es) if using dataclasses or attrs .","title":"Using Inheritance"},{"location":"data-models/#fixing-employeelist","text":"Don't forget to have EmployeeList return a PartialEmployee instance now instead of a dict : def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( PartialEmployee ( first = first . text , last = last . text , position = position . text , ), source = XPath ( \"./a/@href\" ) . match_one ( details ), )","title":"Fixing EmployeeList"},{"location":"data-models/#overriding-default-values","text":"Sometimes you may want to override default values (especially useful if the behavior of the second scrape varies on data from the first).","title":"Overriding Default Values"},{"location":"data-models/#via-command-line","text":"The --data flag to spatula test allows overriding input values with key=value pairs. $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" --data first = John --data last = Neptune EmployeeDetail expects input (PartialEmployee): first: John last: Neptune position: ~position INFO:ex03_data.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 Employee(first='John', last='Neptune', position='~position', marital_status='Married', children='1', hired='3/6/1963') Alternately, --interactive will prompt for input data.","title":"via Command Line"},{"location":"data-models/#via-example_input","text":"You can also provide the example_input attribute on the class in question. This value is assumed to be of type example_type . For example: class EmployeeDetail ( HtmlPage ): input_type = PartialEmployee example_input = PartialEmployee ( \"John\" , \"Neptune\" , \"Engineer\" )","title":"via example_input"},{"location":"data-models/#example_source","text":"Like the above example_input you can define example_source to set a default value for the --source parameter when invoking spatula test class EmployeeDetail ( HtmlPage ): input_type = PartialEmployee example_input = PartialEmployee ( \"John\" , \"Neptune\" , \"Engineer\" ) example_source = \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" Warning Be sure not to confuse source with example_source . The former is used whenever the class is invoked without a source parameter, while example_source is only used when running spatula test .","title":"example_source"},{"location":"data-models/#get_source_from_input","text":"It is not uncommon to want to capture a URL as part of the data and then use that URL as the next source . Let's go ahead and modify PartialEmployee to collect a URL: dataclasses @dataclass class PartialEmployee : first : str last : str position : str url : str attrs @attr . s ( auto_attribs = True ) class PartialEmployee : first : str last : str position : str url : str pydantic class PartialEmployee ( BaseModel ): first : str last : str position : str url : str And then we'll modify EmployeeList.process_item to capture this URL, and stop providing a redundant source : def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( PartialEmployee ( first = first . text , last = last . text , position = position . text , url = XPath ( \"./a/@href\" ) . match_one ( details ), ), ) And finally, add a get_source_from_input method to EmployeeDetail (as well as updating the other uses of Employee to have URL): class EmployeeDetail ( HtmlPage ): input_type = PartialEmployee example_input = PartialEmployee ( \"John\" , \"Neptune\" , \"Engineer\" , \"https://yoyodyne-propulsion.herokuapp.com/staff/1\" , ) def get_source_from_input ( self ): return self . input . url def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return Employee ( first = self . input . first , last = self . input . last , position = self . input . position , url = self . input . url , marital_status = marital_status . text , children = children . text , hired = hired . text , ) Of course, if you have a more complex situation you can do whatever you like in get_source_from_input .","title":"get_source_from_input"},{"location":"data-models/#data-models-as-output","text":"When running spatula scrape data is written to disk as JSON. The exact method of obtaining that JSON varies a bit depending on what type of output you have: Raw dict : Output will match exactly. dataclasses : dataclass.asdict will be used. attrs : attr.asdict will be used to obtain a serializable representation. pydantic : the model's dict() method will be used. By default the filename will be a UUID, but if you wish to provide your own filename you can add a get_filename method to your model. Warning When providing get_filename be sure that your filenames are still unique (you may wish to still incorporate a UUID if you don't have a key you're sure is unique). spatula does not check for this, so you may overwrite data if your get_filename function does not guarantee uniqueness.","title":"Data Models As Output"},{"location":"philosophy/","text":"Design Philosophy \u00b6 spatula came out of several years of maintaining a very large, community-driven scraper project, Open States . While several of the decisions made are best understood through this lens, the goal of spatula is to remain lightweight so that even simple projects can benefit from its structure. Common Problems \u00b6 If you've written a few web scrapers, you're likely to have run into a few common issues: Scrapers are inherently messy as they tend to reflect the cleanliness (or lack thereof) of the underlying site. Scrapers are often written and then left alone for months or years, only needing maintenance when the underlying site changes. Some scrapes take hours to complete, making the development cycle for testing changes and fixes quite difficult. Goals \u00b6 The framework should make it as easy as possible to get started and write clean scraper code. While it is often easy, and tempting, to write a scraper as a dirty one-off script, spatula makes an attempt to provide an easy framework that most scrapers fit within without additional overhead. This reflects the reality that many scraper projects start small but grow quickly, so reaching for a heavyweight tool from the start often does not seem practical. The initial overhead imposed by the framework should be as light as possible, providing benefits even for authors that do not wish to use every feature available to them. The framework should make it easy to read code that was written, with as many of the underlying assumptions of the scraper presented as clearly as possible for future maintenance. By encouraging users to structure their scrapers in a readable way, scrapers will be easier to read later, and issues/maintenance can often be identified with specific components instead of forcing maintainers to comprehend a single gnarly script. Iterative development of scrapers should be the norm. It should be possible to test changes to a single part of the scrape without running the entire process. The user shouldn't have to comment out code or add temporary debug statements to do something that is extremely common when authoring scrapers. Leverage modern Python to improve the experience. There are some great things about Python 3 that we can leverage to make writing & maintaining scrapers easier. To that end, spatula is fully type-annotated, and integrates well with dataclasses , attrs , and pydantic . Pages & Page Roles \u00b6 A key component of using spatula is thinking of the scraper in terms of types of pages. For each type of page you encounter, you'll write a subclass of Page to extract the data from it. Tip If you're familiar with MVC frameworks, a good way to think of this concept is the inverse of a view: a Page takes some kind of presentation (e.g. an HTML page or CSV file) and converts it back to the underlying data that it is comprised of. There are a few types of pages we generally encounter when scraping sites. How we write and use our Page subclasses will depend upon which of these roles the page fulfills. The two most common are what we can call list and detail pages. List pages contain some kind of list of information, perhaps all of the members of a given legislature. They'll often provide links to our second type of page, which we'll call a detail page. Detail pages in our example would contain additional information on a single legislator. A detail page may also need to get information from additional pages (for instance, if the legislator's biographical & contact information are split across two pages). These pages are handled nearly identically, and still regarded as detail pages . The final role is what we call an augmentation page . These provide data with which we want to augment the entire data set. (An example would be a separate page that is a photo directory of all legislators. We'll need to scrape this and match the photos to the legislators we scraped from the list/detail pages.) Augmentation pages are typically handled via dependencies .","title":"Design Philosophy"},{"location":"philosophy/#design-philosophy","text":"spatula came out of several years of maintaining a very large, community-driven scraper project, Open States . While several of the decisions made are best understood through this lens, the goal of spatula is to remain lightweight so that even simple projects can benefit from its structure.","title":"Design Philosophy"},{"location":"philosophy/#common-problems","text":"If you've written a few web scrapers, you're likely to have run into a few common issues: Scrapers are inherently messy as they tend to reflect the cleanliness (or lack thereof) of the underlying site. Scrapers are often written and then left alone for months or years, only needing maintenance when the underlying site changes. Some scrapes take hours to complete, making the development cycle for testing changes and fixes quite difficult.","title":"Common Problems"},{"location":"philosophy/#goals","text":"The framework should make it as easy as possible to get started and write clean scraper code. While it is often easy, and tempting, to write a scraper as a dirty one-off script, spatula makes an attempt to provide an easy framework that most scrapers fit within without additional overhead. This reflects the reality that many scraper projects start small but grow quickly, so reaching for a heavyweight tool from the start often does not seem practical. The initial overhead imposed by the framework should be as light as possible, providing benefits even for authors that do not wish to use every feature available to them. The framework should make it easy to read code that was written, with as many of the underlying assumptions of the scraper presented as clearly as possible for future maintenance. By encouraging users to structure their scrapers in a readable way, scrapers will be easier to read later, and issues/maintenance can often be identified with specific components instead of forcing maintainers to comprehend a single gnarly script. Iterative development of scrapers should be the norm. It should be possible to test changes to a single part of the scrape without running the entire process. The user shouldn't have to comment out code or add temporary debug statements to do something that is extremely common when authoring scrapers. Leverage modern Python to improve the experience. There are some great things about Python 3 that we can leverage to make writing & maintaining scrapers easier. To that end, spatula is fully type-annotated, and integrates well with dataclasses , attrs , and pydantic .","title":"Goals"},{"location":"philosophy/#pages-page-roles","text":"A key component of using spatula is thinking of the scraper in terms of types of pages. For each type of page you encounter, you'll write a subclass of Page to extract the data from it. Tip If you're familiar with MVC frameworks, a good way to think of this concept is the inverse of a view: a Page takes some kind of presentation (e.g. an HTML page or CSV file) and converts it back to the underlying data that it is comprised of. There are a few types of pages we generally encounter when scraping sites. How we write and use our Page subclasses will depend upon which of these roles the page fulfills. The two most common are what we can call list and detail pages. List pages contain some kind of list of information, perhaps all of the members of a given legislature. They'll often provide links to our second type of page, which we'll call a detail page. Detail pages in our example would contain additional information on a single legislator. A detail page may also need to get information from additional pages (for instance, if the legislator's biographical & contact information are split across two pages). These pages are handled nearly identically, and still regarded as detail pages . The final role is what we call an augmentation page . These provide data with which we want to augment the entire data set. (An example would be a separate page that is a photo directory of all legislators. We'll need to scrape this and match the photos to the legislators we scraped from the list/detail pages.) Augmentation pages are typically handled via dependencies .","title":"Pages &amp; Page Roles"},{"location":"reference/","text":"API Reference \u00b6 Pages \u00b6 Page \u00b6 Base class for all Page scrapers, used for scraping information from a single type of page. For details on how these methods are called, it may be helpful to read Anatomy of a Scrape . Attributes source Can be set on subclasses of Page to define the initial HTTP request that the page will handle in its process_response method. For simple GET requests, source can be a string. URL should be used for more advanced use cases. response requests.Response object available if access is needed to the raw response for any reason. input Instance of data being passed upon instantiation of this page. Must be of type input_type . input_type dataclass , attrs class, or pydantic model. If set will be used to prompt for and/or validate self.input example_input Instance of input_type to be used when invoking spatula test . example_source Source to fetch when invokking spatula test . dependencies Dictionary mapping of names to Page objects that will be available before process_page . For example: class EmployeeDetail ( HtmlPage ): dependencies = { \"awards\" : AwardsPage ()} Means that before EmployeeDetail.process_page is called, it is guaranteed to have the output from AwardsPage available as self.awards . See Specifying Dependencies for a more detailed explanation. Methods do_scrape ( self , scraper = None ) \u00b6 yield results from this page and any subpages Parameters: Name Type Description Default scraper Optional[scrapelib.Scraper] Optional scrapelib.Scraper instance to use for running scrape. None Returns: Type Description Iterable[Any] Generator yielding results from the scrape. get_next_source ( self ) \u00b6 To be overriden for paginated pages. Return a URL or valid source to fetch the next page, None if there isn't one. get_source_from_input ( self ) \u00b6 To be overridden. Convert self.input to a Source object such as a URL . postprocess_response ( self ) \u00b6 To be overridden. This is called after source.get_response but before self.process_page. process_error_response ( self , exception ) \u00b6 To be overridden. This is called after source.get_response if an exception is raised. process_page ( self ) \u00b6 To be overridden. Return data extracted from this page and this page alone. HtmlPage \u00b6 Page that automatically handles parsing and normalizing links in an HTML response. Attributes root lxml.etree.Element object representing the root element (e.g. <html> ) on the page. Can use the normal lxml methods (such as cssselect and getchildren ), or use this element as the target of a Selector subclass. JsonPage \u00b6 Page that automatically handles parsing a JSON response. Attributes data JSON data from response. (same as self.response.json() ) PdfPage \u00b6 Page that automatically handles converting a PDF response to text using pdftotext . Attributes preserve_layout set to True on derived class if you want the conversion function to use pdftotext's -layout option to attempt to preserve the layout of text. ( False by default) text UTF8 text extracted by pdftotext. XmlPage \u00b6 Page that automatically handles parsing a XML response. Attributes root lxml.etree.Element object representing the root XML element on the page. ListPages \u00b6 ListPage \u00b6 Base class for common pattern of extracting many homogenous items from one page. Instead of overriding process_response , subclasses should provide a process_item . Methods process_item ( self , item ) \u00b6 To be overridden. Called once per subitem on page, as defined by the particular subclass being used. skip ( self , msg = '' ) \u00b6 Can be called from within process_item to skip a given item. Typically used if there is some known bad data. CsvListPage \u00b6 Processes each row in a CSV (after the first, assumed to be headers) as an item with process_item . ExcelListPage \u00b6 Processes each row in an Excel file as an item with process_item . HtmlListPage \u00b6 Selects homogenous items from HTML page using selector and passes them to process_item . Attributes selector Selector subclass which matches list of homogenous elements to process. (e.g. CSS(\"tbody tr\") ) JsonListPage \u00b6 Processes each element in a JSON list as an item with process_item . XmlListPage \u00b6 Selects homogenous items from XML document using selector and passes them to process_item . Attributes selector Selector subclass which matches list of homogenous elements to process. (e.g. XPath(\"//item\") ) Selectors \u00b6 Selector \u00b6 Base class implementing Selector interface. match ( self , element , * , min_items = None , max_items = None , num_items = None ) \u00b6 Return all matches of the given selector within element . If the number of elements matched is outside the prescribed boundaries, a SelectorError is raised. Parameters: Name Type Description Default element _Element The element to match within. When used from within a Page will usually be self.root . required min_items Optional[int] A minimum number of items to match. None max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None match_one ( self , element ) \u00b6 Return exactly one match. Parameters: Name Type Description Default element _Element Element to search within. required CSS \u00b6 Utilize CSS-style selectors. Parameters: Name Type Description Default css_selector str CSS selector expression to use. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None SimilarLink \u00b6 Match links that fit a provided pattern. Parameters: Name Type Description Default pattern str Regular expression for link hrefs. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None XPath \u00b6 Utilize XPath selectors. Parameters: Name Type Description Default xpath str XPath expression to use. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None Sources \u00b6 URL \u00b6 Defines a resource to fetch via URL, particularly useful for handling non-GET requests. Parameters: Name Type Description Default url str URL to fetch required method str HTTP method to use, defaults to \"GET\" 'GET' data dict POST data to include in request body. None headers dict dictionary of HTTP headers to set for the request. None NullSource \u00b6 Special class to set as a page's source to indicate no HTTP request needs to be performed.","title":"API Reference"},{"location":"reference/#api-reference","text":"","title":"API Reference"},{"location":"reference/#pages","text":"","title":"Pages"},{"location":"reference/#page","text":"Base class for all Page scrapers, used for scraping information from a single type of page. For details on how these methods are called, it may be helpful to read Anatomy of a Scrape . Attributes source Can be set on subclasses of Page to define the initial HTTP request that the page will handle in its process_response method. For simple GET requests, source can be a string. URL should be used for more advanced use cases. response requests.Response object available if access is needed to the raw response for any reason. input Instance of data being passed upon instantiation of this page. Must be of type input_type . input_type dataclass , attrs class, or pydantic model. If set will be used to prompt for and/or validate self.input example_input Instance of input_type to be used when invoking spatula test . example_source Source to fetch when invokking spatula test . dependencies Dictionary mapping of names to Page objects that will be available before process_page . For example: class EmployeeDetail ( HtmlPage ): dependencies = { \"awards\" : AwardsPage ()} Means that before EmployeeDetail.process_page is called, it is guaranteed to have the output from AwardsPage available as self.awards . See Specifying Dependencies for a more detailed explanation. Methods","title":"Page"},{"location":"reference/#spatula.pages.Page.do_scrape","text":"yield results from this page and any subpages Parameters: Name Type Description Default scraper Optional[scrapelib.Scraper] Optional scrapelib.Scraper instance to use for running scrape. None Returns: Type Description Iterable[Any] Generator yielding results from the scrape.","title":"do_scrape()"},{"location":"reference/#spatula.pages.Page.get_next_source","text":"To be overriden for paginated pages. Return a URL or valid source to fetch the next page, None if there isn't one.","title":"get_next_source()"},{"location":"reference/#spatula.pages.Page.get_source_from_input","text":"To be overridden. Convert self.input to a Source object such as a URL .","title":"get_source_from_input()"},{"location":"reference/#spatula.pages.Page.postprocess_response","text":"To be overridden. This is called after source.get_response but before self.process_page.","title":"postprocess_response()"},{"location":"reference/#spatula.pages.Page.process_error_response","text":"To be overridden. This is called after source.get_response if an exception is raised.","title":"process_error_response()"},{"location":"reference/#spatula.pages.Page.process_page","text":"To be overridden. Return data extracted from this page and this page alone.","title":"process_page()"},{"location":"reference/#htmlpage","text":"Page that automatically handles parsing and normalizing links in an HTML response. Attributes root lxml.etree.Element object representing the root element (e.g. <html> ) on the page. Can use the normal lxml methods (such as cssselect and getchildren ), or use this element as the target of a Selector subclass.","title":"HtmlPage"},{"location":"reference/#jsonpage","text":"Page that automatically handles parsing a JSON response. Attributes data JSON data from response. (same as self.response.json() )","title":"JsonPage"},{"location":"reference/#pdfpage","text":"Page that automatically handles converting a PDF response to text using pdftotext . Attributes preserve_layout set to True on derived class if you want the conversion function to use pdftotext's -layout option to attempt to preserve the layout of text. ( False by default) text UTF8 text extracted by pdftotext.","title":"PdfPage"},{"location":"reference/#xmlpage","text":"Page that automatically handles parsing a XML response. Attributes root lxml.etree.Element object representing the root XML element on the page.","title":"XmlPage"},{"location":"reference/#listpages","text":"","title":"ListPages"},{"location":"reference/#listpage","text":"Base class for common pattern of extracting many homogenous items from one page. Instead of overriding process_response , subclasses should provide a process_item . Methods","title":"ListPage"},{"location":"reference/#spatula.pages.ListPage.process_item","text":"To be overridden. Called once per subitem on page, as defined by the particular subclass being used.","title":"process_item()"},{"location":"reference/#spatula.pages.ListPage.skip","text":"Can be called from within process_item to skip a given item. Typically used if there is some known bad data.","title":"skip()"},{"location":"reference/#csvlistpage","text":"Processes each row in a CSV (after the first, assumed to be headers) as an item with process_item .","title":"CsvListPage"},{"location":"reference/#excellistpage","text":"Processes each row in an Excel file as an item with process_item .","title":"ExcelListPage"},{"location":"reference/#htmllistpage","text":"Selects homogenous items from HTML page using selector and passes them to process_item . Attributes selector Selector subclass which matches list of homogenous elements to process. (e.g. CSS(\"tbody tr\") )","title":"HtmlListPage"},{"location":"reference/#jsonlistpage","text":"Processes each element in a JSON list as an item with process_item .","title":"JsonListPage"},{"location":"reference/#xmllistpage","text":"Selects homogenous items from XML document using selector and passes them to process_item . Attributes selector Selector subclass which matches list of homogenous elements to process. (e.g. XPath(\"//item\") )","title":"XmlListPage"},{"location":"reference/#selectors","text":"","title":"Selectors"},{"location":"reference/#selector","text":"Base class implementing Selector interface.","title":"Selector"},{"location":"reference/#spatula.selectors.Selector.match","text":"Return all matches of the given selector within element . If the number of elements matched is outside the prescribed boundaries, a SelectorError is raised. Parameters: Name Type Description Default element _Element The element to match within. When used from within a Page will usually be self.root . required min_items Optional[int] A minimum number of items to match. None max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None","title":"match()"},{"location":"reference/#spatula.selectors.Selector.match_one","text":"Return exactly one match. Parameters: Name Type Description Default element _Element Element to search within. required","title":"match_one()"},{"location":"reference/#css","text":"Utilize CSS-style selectors. Parameters: Name Type Description Default css_selector str CSS selector expression to use. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None","title":"CSS"},{"location":"reference/#similarlink","text":"Match links that fit a provided pattern. Parameters: Name Type Description Default pattern str Regular expression for link hrefs. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None","title":"SimilarLink"},{"location":"reference/#xpath","text":"Utilize XPath selectors. Parameters: Name Type Description Default xpath str XPath expression to use. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None","title":"XPath"},{"location":"reference/#sources","text":"","title":"Sources"},{"location":"reference/#url","text":"Defines a resource to fetch via URL, particularly useful for handling non-GET requests. Parameters: Name Type Description Default url str URL to fetch required method str HTTP method to use, defaults to \"GET\" 'GET' data dict POST data to include in request body. None headers dict dictionary of HTTP headers to set for the request. None","title":"URL"},{"location":"reference/#nullsource","text":"Special class to set as a page's source to indicate no HTTP request needs to be performed.","title":"NullSource"},{"location":"scraper-basics/","text":"A First Scraper \u00b6 This guide contains quick examples of how you could scrape a small list of employees of the fictional Yoyodyne Propulsion Systems , a site developed for demonstrating web scraping. This will give you an idea of what it looks like to write a scraper using spatula . Scraping a List Page \u00b6 It is fairly common for a scrape to begin on some sort of directory or listing page. We'll start by scraping the staff list on https://yoyodyne-propulsion.herokuapp.com/staff This page has a fairly simple HTML table with four columns: < table id = \"employees\" > < thead > < tr > < th > First Name </ th > < th > Last Name </ th > < th > Position Name </ th > < th > &nbsp; </ th > </ tr > </ thead > < tbody > < tr > < td > John </ td > < td > Barnett </ td > < td > Scheduling </ td > < td >< a href = \"/staff/52\" > Details </ a ></ td > </ tr > < tr > < td > John </ td > < td > Bigboot\u00e9 </ td > < td > Executive Vice President </ td > < td >< a href = \"/staff/2\" > Details </ a ></ td > </ tr > ...continues... spatula provides a special interface for these cases. See below how we process each matching link by deriving from a HtmlListPage and providing a selector as well as a process_item method. Open a file named quickstart.py and add the following code: # imports we'll use in this example from spatula import ( HtmlPage , HtmlListPage , CSS , XPath , SelectorError ) class EmployeeList ( HtmlListPage ): source = \"https://yoyodyne-propulsion.herokuapp.com/staff\" # each row represents an employee selector = CSS ( \"#employees tbody tr\" ) def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return dict ( first = first . text , last = last . text , position = position . text , ) One concept in spatula is that we typically write one class per type of page we encounter. This class defines the logic to process the employee list page. This class will turn each row on the page into a dictionary with the 'first', 'last', and 'position' keys. It can be tested from the command line like: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: {'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} 2: {'first': 'John', 'last': 'Bigboot\u00e9', 'position': 'Executive Vice President'} 3: {'first': 'John', 'last': 'Camp', 'position': 'Human Resources'} ... 10: {'first': 'John', 'last': 'Fish', 'position': 'Marine R&D'} The spatula test command lets us quickly see the output of the part of the scraper we're working on. You may notice that we're only grabbing the first page for now, we'll come back in a bit to handle pagination. Scraping a Single Page \u00b6 Employees have a few more details not included in the table on pages like https://yoyodyne-propulsion.herokuapp.com/staff/52 . We're going to pull some data elements from the page that look like: < h2 class = \"section\" > Employee Details for John Barnett </ h2 > < div class = \"section\" > < dl > < dt > Position </ dt > < dd id = \"position\" > Scheduling </ dd > < dt > Marital Status </ dt > < dd id = \"status\" > Married </ dd > < dt > Number of Children </ dt > < dd id = \"children\" > 1 </ dd > < dt > Hired </ dt > < dd id = \"hired\" > 3/6/1963 </ dd > </ dl > </ div > To demonstrate extracting the details from this page, we'll write a small class to handle individual employee pages. Whereas before we used HtmlListPage and overrode process_item , this time we'll subclass HtmlPage , and override the process_page function. class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , ) This will extract the elements from the page and return them in a dictionary. It can be tested from the command line like: $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 {'children': '1', 'hired': '3/6/1963', 'marital_status': 'Married'} One thing to note is that since we didn't define a single source attribute like we did in EmployeeList , we need to pass one on the command line with --source . This lets you quickly try your scraper against multiple variants of a page as needed. Chaining Pages Together \u00b6 Most moderately complex sites will require chaining data together from multiple pages to get a complete object. Let's revisit EmployeeList and have it return instances of EmployeeDetail to tell spatula that more work is needed: class EmployeeList ( HtmlListPage ): # by providing this here, it can be omitted on the command line # useful in cases where the scraper is only meant for one page source = \"https://yoyodyne-propulsion.herokuapp.com/staff\" # each row represents an employee selector = CSS ( \"#employees tbody tr\" ) def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( dict ( first = first . text , last = last . text , position = position . text , ), source = XPath ( \"./a/@href\" ) . match_one ( details ), ) And we can revisit EmployeeDetail to tell it to combine the data it collects with the data passed in from the parent page: class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape, # in this case a dict we can expand here ** self . input , ) Now a run looks like: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: EmployeeDetail(input={'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} source=https://yoyodyne-propulsion.herokuapp.com/staff) 2: EmployeeDetail(input={'first': 'John', 'last': 'Bigboot\u00e9', 'position': 'Executive Vice President'} source=https:/yoyodyne-propulsion.herokuapp.com/staff/2) ... 10: EmployeeDetail(input={'first': 'John', 'last': 'Fish', 'position': 'Marine R&D'} source=https:/yoyodyne-propulsion.herokuapp.com/staff/20) By default, spatula test just shows the result of the page you're working on, but you can see that it is now returning page objects with the data and a source set. Running a Scrape \u00b6 Now that we're happy with our individual page scrapers, we can run the full scrape and write the data to disk. For this we use the spatula scrape command: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/2 ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/100 INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/101 success: wrote 10 objects to _scrapes/2021-06-03/001 And now our scraped data is on disk, ready for you to use! If you look at a data file you'll see that it has the full data for an individual: { \"marital_status\" : \"Single\" , \"children\" : \"0\" , \"hired\" : \"9/9/1963\" , \"first\" : \"John\" , \"last\" : \"Omar\" , \"position\" : \"Imports & Exports\" } Using spatula Within Other Scripts \u00b6 Perhaps you don't want to write your output to disk. If you want to post-process your data further or use it as part of a larger pipeline a page's do_scrape method lets you do just that. It returns a generator that you can use to process items as you see fit. For example: page = EmployeeList () for e in page . do_scrape (): print ( e ) You an do whatever you wish with these results, output them in a custom format, save them to your database, etc. Pagination \u00b6 While writing the list page we ignored pagination, let's go ahead and add it now. If we override the get_next_source method on our EmployeeLList class, spatula will continue to the next page once it has called process_item on all elements on the current page. # add this within EmployeeList def get_next_source ( self ): try : return XPath ( \"//a[contains(text(), 'Next')]/@href\" ) . match_one ( self . root ) except SelectorError : pass You'll notice the output of spatula test quickstart.EmployeeList has now changed: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: {'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} ... paginating for EmployeeList source=https://yoyodyne-propulsion.herokuapp.com/staff?page=2 INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff?page=2 ... 45: EmployeeDetail(input={'first': 'John', 'last': 'Ya Ya', 'position': 'Computer Design Specialist'} source=https://yoyodyne-propulsion.herokuapp.com/staff/101) Error Handling \u00b6 Now that we're grabbing all 45 employees, kick off another full scrape: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/404 Traceback (most recent call last): ... scrapelib.HTTPError: 404 while retrieving https://yoyodyne-propulsion.herokuapp.com/staff/404 An error! It turns out that one of the employee pages isn't loading correctly. Sometimes it is best to let these errors propagate so you can try to fix the broken scraper. Other times it makes more sense to handle the error and move on. If you wish to do that, you can override process_error_response . Add the following to EmployeeDetail : def process_error_response ( self , exception ): # every Page subclass has a built-in logger object self . logger . warning ( exception ) Run the scrape again to see this in action: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff ... WARNING:quickstart.EmployeeDetail:404 while retrieving https://yoyodyne-propulsion.herokuapp.com/staff/404 ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/101 success: wrote 44 objects to _scrapes/2021-06-03/002","title":"A First Scraper"},{"location":"scraper-basics/#a-first-scraper","text":"This guide contains quick examples of how you could scrape a small list of employees of the fictional Yoyodyne Propulsion Systems , a site developed for demonstrating web scraping. This will give you an idea of what it looks like to write a scraper using spatula .","title":"A First Scraper"},{"location":"scraper-basics/#scraping-a-list-page","text":"It is fairly common for a scrape to begin on some sort of directory or listing page. We'll start by scraping the staff list on https://yoyodyne-propulsion.herokuapp.com/staff This page has a fairly simple HTML table with four columns: < table id = \"employees\" > < thead > < tr > < th > First Name </ th > < th > Last Name </ th > < th > Position Name </ th > < th > &nbsp; </ th > </ tr > </ thead > < tbody > < tr > < td > John </ td > < td > Barnett </ td > < td > Scheduling </ td > < td >< a href = \"/staff/52\" > Details </ a ></ td > </ tr > < tr > < td > John </ td > < td > Bigboot\u00e9 </ td > < td > Executive Vice President </ td > < td >< a href = \"/staff/2\" > Details </ a ></ td > </ tr > ...continues... spatula provides a special interface for these cases. See below how we process each matching link by deriving from a HtmlListPage and providing a selector as well as a process_item method. Open a file named quickstart.py and add the following code: # imports we'll use in this example from spatula import ( HtmlPage , HtmlListPage , CSS , XPath , SelectorError ) class EmployeeList ( HtmlListPage ): source = \"https://yoyodyne-propulsion.herokuapp.com/staff\" # each row represents an employee selector = CSS ( \"#employees tbody tr\" ) def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return dict ( first = first . text , last = last . text , position = position . text , ) One concept in spatula is that we typically write one class per type of page we encounter. This class defines the logic to process the employee list page. This class will turn each row on the page into a dictionary with the 'first', 'last', and 'position' keys. It can be tested from the command line like: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: {'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} 2: {'first': 'John', 'last': 'Bigboot\u00e9', 'position': 'Executive Vice President'} 3: {'first': 'John', 'last': 'Camp', 'position': 'Human Resources'} ... 10: {'first': 'John', 'last': 'Fish', 'position': 'Marine R&D'} The spatula test command lets us quickly see the output of the part of the scraper we're working on. You may notice that we're only grabbing the first page for now, we'll come back in a bit to handle pagination.","title":"Scraping a List Page"},{"location":"scraper-basics/#scraping-a-single-page","text":"Employees have a few more details not included in the table on pages like https://yoyodyne-propulsion.herokuapp.com/staff/52 . We're going to pull some data elements from the page that look like: < h2 class = \"section\" > Employee Details for John Barnett </ h2 > < div class = \"section\" > < dl > < dt > Position </ dt > < dd id = \"position\" > Scheduling </ dd > < dt > Marital Status </ dt > < dd id = \"status\" > Married </ dd > < dt > Number of Children </ dt > < dd id = \"children\" > 1 </ dd > < dt > Hired </ dt > < dd id = \"hired\" > 3/6/1963 </ dd > </ dl > </ div > To demonstrate extracting the details from this page, we'll write a small class to handle individual employee pages. Whereas before we used HtmlListPage and overrode process_item , this time we'll subclass HtmlPage , and override the process_page function. class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , ) This will extract the elements from the page and return them in a dictionary. It can be tested from the command line like: $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 {'children': '1', 'hired': '3/6/1963', 'marital_status': 'Married'} One thing to note is that since we didn't define a single source attribute like we did in EmployeeList , we need to pass one on the command line with --source . This lets you quickly try your scraper against multiple variants of a page as needed.","title":"Scraping a Single Page"},{"location":"scraper-basics/#chaining-pages-together","text":"Most moderately complex sites will require chaining data together from multiple pages to get a complete object. Let's revisit EmployeeList and have it return instances of EmployeeDetail to tell spatula that more work is needed: class EmployeeList ( HtmlListPage ): # by providing this here, it can be omitted on the command line # useful in cases where the scraper is only meant for one page source = \"https://yoyodyne-propulsion.herokuapp.com/staff\" # each row represents an employee selector = CSS ( \"#employees tbody tr\" ) def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( dict ( first = first . text , last = last . text , position = position . text , ), source = XPath ( \"./a/@href\" ) . match_one ( details ), ) And we can revisit EmployeeDetail to tell it to combine the data it collects with the data passed in from the parent page: class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape, # in this case a dict we can expand here ** self . input , ) Now a run looks like: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: EmployeeDetail(input={'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} source=https://yoyodyne-propulsion.herokuapp.com/staff) 2: EmployeeDetail(input={'first': 'John', 'last': 'Bigboot\u00e9', 'position': 'Executive Vice President'} source=https:/yoyodyne-propulsion.herokuapp.com/staff/2) ... 10: EmployeeDetail(input={'first': 'John', 'last': 'Fish', 'position': 'Marine R&D'} source=https:/yoyodyne-propulsion.herokuapp.com/staff/20) By default, spatula test just shows the result of the page you're working on, but you can see that it is now returning page objects with the data and a source set.","title":"Chaining Pages Together"},{"location":"scraper-basics/#running-a-scrape","text":"Now that we're happy with our individual page scrapers, we can run the full scrape and write the data to disk. For this we use the spatula scrape command: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/2 ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/100 INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/101 success: wrote 10 objects to _scrapes/2021-06-03/001 And now our scraped data is on disk, ready for you to use! If you look at a data file you'll see that it has the full data for an individual: { \"marital_status\" : \"Single\" , \"children\" : \"0\" , \"hired\" : \"9/9/1963\" , \"first\" : \"John\" , \"last\" : \"Omar\" , \"position\" : \"Imports & Exports\" }","title":"Running a Scrape"},{"location":"scraper-basics/#using-spatula-within-other-scripts","text":"Perhaps you don't want to write your output to disk. If you want to post-process your data further or use it as part of a larger pipeline a page's do_scrape method lets you do just that. It returns a generator that you can use to process items as you see fit. For example: page = EmployeeList () for e in page . do_scrape (): print ( e ) You an do whatever you wish with these results, output them in a custom format, save them to your database, etc.","title":"Using spatula Within Other Scripts"},{"location":"scraper-basics/#pagination","text":"While writing the list page we ignored pagination, let's go ahead and add it now. If we override the get_next_source method on our EmployeeLList class, spatula will continue to the next page once it has called process_item on all elements on the current page. # add this within EmployeeList def get_next_source ( self ): try : return XPath ( \"//a[contains(text(), 'Next')]/@href\" ) . match_one ( self . root ) except SelectorError : pass You'll notice the output of spatula test quickstart.EmployeeList has now changed: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: {'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} ... paginating for EmployeeList source=https://yoyodyne-propulsion.herokuapp.com/staff?page=2 INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff?page=2 ... 45: EmployeeDetail(input={'first': 'John', 'last': 'Ya Ya', 'position': 'Computer Design Specialist'} source=https://yoyodyne-propulsion.herokuapp.com/staff/101)","title":"Pagination"},{"location":"scraper-basics/#error-handling","text":"Now that we're grabbing all 45 employees, kick off another full scrape: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/404 Traceback (most recent call last): ... scrapelib.HTTPError: 404 while retrieving https://yoyodyne-propulsion.herokuapp.com/staff/404 An error! It turns out that one of the employee pages isn't loading correctly. Sometimes it is best to let these errors propagate so you can try to fix the broken scraper. Other times it makes more sense to handle the error and move on. If you wish to do that, you can override process_error_response . Add the following to EmployeeDetail : def process_error_response ( self , exception ): # every Page subclass has a built-in logger object self . logger . warning ( exception ) Run the scrape again to see this in action: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff ... WARNING:quickstart.EmployeeDetail:404 while retrieving https://yoyodyne-propulsion.herokuapp.com/staff/404 ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/101 success: wrote 44 objects to _scrapes/2021-06-03/002","title":"Error Handling"}]}