{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About Spatula \u00b6 spatula is a modern Python library for writing maintainable web scrapers. Source: https://github.com/jamesturk/spatula Documentation: https://jamesturk.github.io/spatula/ Features \u00b6 Page-oriented design : Encourages writing understandable & maintainable scrapers. Not Just HTML : Provides built in handlers for common data formats including CSV, JSON, XML, PDF, and Excel. Or write your own. Fast HTML parsing : Uses lxml.html for fast, consistent, and reliable parsing of HTML. Flexible Data Model Support : Compatible with dataclasses , attrs , pydantic , or bring your own data model classes for storing & validating your scraped data. CLI Tools : Offers several CLI utilities that can help streamline development & testing cycle. Fully Typed : Makes full use of Python 3 type annotations. Installation \u00b6 spatula is on PyPI, and can be installed via any standard package management tool: poetry add spatula or: pip install spatula","title":"About Spatula"},{"location":"#about-spatula","text":"spatula is a modern Python library for writing maintainable web scrapers. Source: https://github.com/jamesturk/spatula Documentation: https://jamesturk.github.io/spatula/","title":"About Spatula"},{"location":"#features","text":"Page-oriented design : Encourages writing understandable & maintainable scrapers. Not Just HTML : Provides built in handlers for common data formats including CSV, JSON, XML, PDF, and Excel. Or write your own. Fast HTML parsing : Uses lxml.html for fast, consistent, and reliable parsing of HTML. Flexible Data Model Support : Compatible with dataclasses , attrs , pydantic , or bring your own data model classes for storing & validating your scraped data. CLI Tools : Offers several CLI utilities that can help streamline development & testing cycle. Fully Typed : Makes full use of Python 3 type annotations.","title":"Features"},{"location":"#installation","text":"spatula is on PyPI, and can be installed via any standard package management tool: poetry add spatula or: pip install spatula","title":"Installation"},{"location":"changelog/","text":"Changelog \u00b6 0.7.0 - 2021-06-04 \u00b6 add spatula scout command make error messages a bit more clear improvements to documentation added more CLI options to control verbosity, user agent, etc. if module cannot be found, search current directory 0.6.0 - 2021-04-12 \u00b6 add full typing to library small bugfixes 0.5.0 - 2021-02-04 \u00b6 add ExcelListPage improve Page.logger and CLI output move to simpler Workflow class spatula scrape can now take the name of a page, will use default Workflow bugfix: inconsistent name for process_error_response 0.4.1 - 2021-02-01 \u00b6 bugfix: dependencies are instantiated from parent page input 0.4.0 - 2021-02-01 \u00b6 restore Python 3.7 compatibility add behavior to handle returning additional Page subclasses to continue scraping add default behavior when Page.input has a url attribute. add PdfPage add page_to_items helper add Page.example_input and Page.example_source for test command add Page.logger for logging allow use of dataclasses in addition to attrs as input objects improve output of HTML elements bugfix: not specifying a page processor on workflow is no longer an error 0.3.0 - 2021-01-18 \u00b6 first documented major release major refactor, inspired by not directly using code from prior versions","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#070-2021-06-04","text":"add spatula scout command make error messages a bit more clear improvements to documentation added more CLI options to control verbosity, user agent, etc. if module cannot be found, search current directory","title":"0.7.0 - 2021-06-04"},{"location":"changelog/#060-2021-04-12","text":"add full typing to library small bugfixes","title":"0.6.0 - 2021-04-12"},{"location":"changelog/#050-2021-02-04","text":"add ExcelListPage improve Page.logger and CLI output move to simpler Workflow class spatula scrape can now take the name of a page, will use default Workflow bugfix: inconsistent name for process_error_response","title":"0.5.0 - 2021-02-04"},{"location":"changelog/#041-2021-02-01","text":"bugfix: dependencies are instantiated from parent page input","title":"0.4.1 - 2021-02-01"},{"location":"changelog/#040-2021-02-01","text":"restore Python 3.7 compatibility add behavior to handle returning additional Page subclasses to continue scraping add default behavior when Page.input has a url attribute. add PdfPage add page_to_items helper add Page.example_input and Page.example_source for test command add Page.logger for logging allow use of dataclasses in addition to attrs as input objects improve output of HTML elements bugfix: not specifying a page processor on workflow is no longer an error","title":"0.4.0 - 2021-02-01"},{"location":"changelog/#030-2021-01-18","text":"first documented major release major refactor, inspired by not directly using code from prior versions","title":"0.3.0 - 2021-01-18"},{"location":"cli/","text":"Command Line Usage \u00b6 spatula provides a command line interface that is useful for iterative development of scrapers. Once installed within your Python environment, spatula can be invoked on the command line. E.g.: (scrape-venv) ~/scrape-proj $ spatula --version spatula, version 0.7.0 Or with poetry: ~/scrape-proj $ poetry run spatula --version spatula, version 0.7.0 The CLI provides four useful subcommands for different stages of development: spatula \u00b6 Usage: spatula [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --version boolean Show the version and exit. False --help boolean Show this message and exit. False scout \u00b6 Run first step of workflow & output data to a JSON file. This command is intended to be used to detect at a first approximation whether or not a full scrape might need to be run. If the first layer detects any changes it is safe to say that the full run will as well. This will work in the common case where a new subpage is added or removed. Of course in more advanced cases this depends upon the first page being scraped (typically a ListPage derivative) surfacing enough information (perhaps a last_updated date) to know whether any of the other pages have been scraped. Usage: spatula scout [OPTIONS] WORKFLOW_NAME Options: Name Type Description Default -o , --output-file text override default output file [default: scout.json]. scout.json -ua , --user-agent text override default user-agent spatula 0.7.0 --rpm integer set requests per minute (default: 60) 60 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --help boolean Show this message and exit. False scrape \u00b6 Run full workflow, and output data to disk. Usage: spatula scrape [OPTIONS] WORKFLOW_NAME Options: Name Type Description Default -o , --output-dir text override default output directory. required -ua , --user-agent text override default user-agent spatula 0.7.0 --rpm integer set requests per minute (default: 60) 60 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --help boolean Show this message and exit. False shell \u00b6 Start a session to interact with a particular page. Usage: spatula shell [OPTIONS] URL Options: Name Type Description Default -X , --verb text set HTTP verb such as POST GET -ua , --user-agent text override default user-agent spatula 0.7.0 --rpm integer set requests per minute (default: 60) 60 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --help boolean Show this message and exit. False test \u00b6 Scrape a single page and see output immediately. This eases the common cycle of making modifications to a scraper, running a scrape (possibly with long-running but irrelevant portions commented out), and comparing output to what is expected. test can also be useful for debugging existing scrapers, you can see exactly what a single step of the scrape is providing, to help narrow down where erroneous data is coming from. Example:: $ spatula test path.to.ClassName --source https://example.com This will run the scraper defined at :py:class: path.to.ClassName against the provided URL. Usage: spatula test [OPTIONS] CLASS_NAME Options: Name Type Description Default --interactive / --no-interactive boolean Interactively prompt for missing data. False -d , --data text Provide input data in name=value pairs. required -s , --source text Provide (or override) source URL required --pagination / --no-pagination boolean Determine whether or not pagination should be followed or one page is enough for testing True -ua , --user-agent text override default user-agent spatula 0.7.0 --rpm integer set requests per minute (default: 60) 60 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --help boolean Show this message and exit. False","title":"Command Line Usage"},{"location":"cli/#command-line-usage","text":"spatula provides a command line interface that is useful for iterative development of scrapers. Once installed within your Python environment, spatula can be invoked on the command line. E.g.: (scrape-venv) ~/scrape-proj $ spatula --version spatula, version 0.7.0 Or with poetry: ~/scrape-proj $ poetry run spatula --version spatula, version 0.7.0 The CLI provides four useful subcommands for different stages of development:","title":"Command Line Usage"},{"location":"cli/#spatula","text":"Usage: spatula [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --version boolean Show the version and exit. False --help boolean Show this message and exit. False","title":"spatula"},{"location":"cli/#scout","text":"Run first step of workflow & output data to a JSON file. This command is intended to be used to detect at a first approximation whether or not a full scrape might need to be run. If the first layer detects any changes it is safe to say that the full run will as well. This will work in the common case where a new subpage is added or removed. Of course in more advanced cases this depends upon the first page being scraped (typically a ListPage derivative) surfacing enough information (perhaps a last_updated date) to know whether any of the other pages have been scraped. Usage: spatula scout [OPTIONS] WORKFLOW_NAME Options: Name Type Description Default -o , --output-file text override default output file [default: scout.json]. scout.json -ua , --user-agent text override default user-agent spatula 0.7.0 --rpm integer set requests per minute (default: 60) 60 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --help boolean Show this message and exit. False","title":"scout"},{"location":"cli/#scrape","text":"Run full workflow, and output data to disk. Usage: spatula scrape [OPTIONS] WORKFLOW_NAME Options: Name Type Description Default -o , --output-dir text override default output directory. required -ua , --user-agent text override default user-agent spatula 0.7.0 --rpm integer set requests per minute (default: 60) 60 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --help boolean Show this message and exit. False","title":"scrape"},{"location":"cli/#shell","text":"Start a session to interact with a particular page. Usage: spatula shell [OPTIONS] URL Options: Name Type Description Default -X , --verb text set HTTP verb such as POST GET -ua , --user-agent text override default user-agent spatula 0.7.0 --rpm integer set requests per minute (default: 60) 60 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --help boolean Show this message and exit. False","title":"shell"},{"location":"cli/#test","text":"Scrape a single page and see output immediately. This eases the common cycle of making modifications to a scraper, running a scrape (possibly with long-running but irrelevant portions commented out), and comparing output to what is expected. test can also be useful for debugging existing scrapers, you can see exactly what a single step of the scrape is providing, to help narrow down where erroneous data is coming from. Example:: $ spatula test path.to.ClassName --source https://example.com This will run the scraper defined at :py:class: path.to.ClassName against the provided URL. Usage: spatula test [OPTIONS] CLASS_NAME Options: Name Type Description Default --interactive / --no-interactive boolean Interactively prompt for missing data. False -d , --data text Provide input data in name=value pairs. required -s , --source text Provide (or override) source URL required --pagination / --no-pagination boolean Determine whether or not pagination should be followed or one page is enough for testing True -ua , --user-agent text override default user-agent spatula 0.7.0 --rpm integer set requests per minute (default: 60) 60 -H , --header text add a header to all requests. example format: 'Accept: application/json' required -v , --verbosity integer override default verbosity for command (0-3) -1 --help boolean Show this message and exit. False","title":"test"},{"location":"next-steps/","text":"Next Steps \u00b6 Pagination \u00b6 While writing the list page we ignored pagination, let's go ahead and add it now. If we override the get_next_source method on our EmployeeLList class, spatula will continue to the next page once it has called process_item on all elements on the current page. # add this within EmployeeList def get_next_source ( self ): try : return XPath ( \"//a[contains(text(), 'Next')]/@href\" ) . match_one ( self . root ) except SelectorError : pass You'll notice the output of spatula test quickstart.EmployeeList has now changed: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: {'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} ... paginating for EmployeeList source=https://yoyodyne-propulsion.herokuapp.com/staff?page=2 INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff?page=2 ... 45: EmployeeDetail(input={'first': 'John', 'last': 'Ya Ya', 'position': 'Computer Design Specialist'} source=https://yoyodyne-propulsion.herokuapp.com/staff/101) Error Handling \u00b6 Now that we're grabbing all 45 employees, kick off another full scrape: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/404 Traceback (most recent call last): ... scrapelib.HTTPError: 404 while retrieving https://yoyodyne-propulsion.herokuapp.com/staff/404 An error! It turns out that one of the employee pages isn't loading correctly. Sometimes it is best to let these errors propagate so you can try to fix the broken scraper. Other times it makes more sense to handle the error and move on. If you wish to do that, you can override process_error_response . Add the following to EmployeeDetail : def process_error_response ( self , exception ): # every Page subclass has a built-in logger object self . logger . warning ( exception ) Run the scrape again to see this in action: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff ... WARNING:quickstart.EmployeeDetail:404 while retrieving https://yoyodyne-propulsion.herokuapp.com/staff/404 ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/101 success: wrote 44 objects to _scrapes/2021-06-03/002 Defining Scraper Input \u00b6 TODO: introduce validation and CLI testing Making Testing Better \u00b6 TODO: add more ways to make testing work Workflows \u00b6 TODO: figure out what if anything to teach on these Now that you've seen the basics, you might want to read a bit more about spatula 's Design Philosophy , or API Reference .","title":"Next Steps"},{"location":"next-steps/#next-steps","text":"","title":"Next Steps"},{"location":"next-steps/#pagination","text":"While writing the list page we ignored pagination, let's go ahead and add it now. If we override the get_next_source method on our EmployeeLList class, spatula will continue to the next page once it has called process_item on all elements on the current page. # add this within EmployeeList def get_next_source ( self ): try : return XPath ( \"//a[contains(text(), 'Next')]/@href\" ) . match_one ( self . root ) except SelectorError : pass You'll notice the output of spatula test quickstart.EmployeeList has now changed: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: {'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} ... paginating for EmployeeList source=https://yoyodyne-propulsion.herokuapp.com/staff?page=2 INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff?page=2 ... 45: EmployeeDetail(input={'first': 'John', 'last': 'Ya Ya', 'position': 'Computer Design Specialist'} source=https://yoyodyne-propulsion.herokuapp.com/staff/101)","title":"Pagination"},{"location":"next-steps/#error-handling","text":"Now that we're grabbing all 45 employees, kick off another full scrape: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/404 Traceback (most recent call last): ... scrapelib.HTTPError: 404 while retrieving https://yoyodyne-propulsion.herokuapp.com/staff/404 An error! It turns out that one of the employee pages isn't loading correctly. Sometimes it is best to let these errors propagate so you can try to fix the broken scraper. Other times it makes more sense to handle the error and move on. If you wish to do that, you can override process_error_response . Add the following to EmployeeDetail : def process_error_response ( self , exception ): # every Page subclass has a built-in logger object self . logger . warning ( exception ) Run the scrape again to see this in action: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff ... WARNING:quickstart.EmployeeDetail:404 while retrieving https://yoyodyne-propulsion.herokuapp.com/staff/404 ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/101 success: wrote 44 objects to _scrapes/2021-06-03/002","title":"Error Handling"},{"location":"next-steps/#defining-scraper-input","text":"TODO: introduce validation and CLI testing","title":"Defining Scraper Input"},{"location":"next-steps/#making-testing-better","text":"TODO: add more ways to make testing work","title":"Making Testing Better"},{"location":"next-steps/#workflows","text":"TODO: figure out what if anything to teach on these Now that you've seen the basics, you might want to read a bit more about spatula 's Design Philosophy , or API Reference .","title":"Workflows"},{"location":"philosophy/","text":"Design Philosophy \u00b6 spatula came out of several years of maintaining a very large, community-driven scraper project, Open States . While several of the decisions made are best understood through this lens, the goal of spatula is to remain lightweight so that even simple projects can benefit from its structure. Common Problems \u00b6 If you've written a few web scrapers, you're likely to have run into a few common issues: Scrapers are inherently messy as they tend to reflect the cleanliness (or lack thereof) of the underlying site. Scrapers are often written and then left alone for months or years, only needing maintenance when the underlying site changes. Some scrapes take hours to complete, making the development cycle for testing changes and fixes quite difficult. Goals \u00b6 The framework should make it as easy as possible to get started and write clean scraper code. While it is often easy, and tempting, to write a scraper as a dirty one-off script, spatula makes an attempt to provide an easy framework that most scrapers fit within without additional overhead. This reflects the reality that many scraper projects start small but grow quickly, so reaching for a heavyweight tool from the start often does not seem practical. The initial overhead imposed by the framework should be as light as possible, providing benefits even for authors that do not wish to use every feature available to them. The framework should make it easy to read code that was written, with as many of the underlying assumptions of the scraper presented as clearly as possible for future maintenance. By encouraging users to structure their scrapers in a readable way, scrapers will be easier to read later, and issues/maintenance can often be identified with specific components instead of forcing maintainers to comprehend a single gnarly script. Iterative development of scrapers should be the norm. It should be possible to test changes to a single part of the scrape without running the entire process. The user shouldn't have to comment out code or add temporary debug statements to do something that is extremely common when authoring scrapers. Leverage modern Python to improve the experience. There are some great things about Python 3 that we can leverage to make writing & maintaining scrapers easier. To that end, spatula is fully type-annotated, and integrates well with dataclasses , attrs , and pydantic . Pages & Page Roles \u00b6 A key component of using spatula is thinking of the scraper in terms of types of \"pages\". For each type of page you encounter, you'll write a subclass of Page to extract the data from it. Note If you're familiar with MVC frameworks, a good way to think of this concept is the inverse of a view: a Page takes some kind of presentation (e.g. an HTML page or CSV file) and converts it back to the underlying data that it is comprised of. There are a few types of pages we generally encounter when scraping sites. How we write and use our Page subclasses will depend upon which of these four roles the page fulfills. The two most common are what we'll call \"listing\" and \"detail\" pages. Listing pages contain some kind of list of information, perhaps all of the members of a given legislature. They'll often provide links to our second type of page, which we'll call a \"detail\" page. A \"detail\" page in our example would contain additional information on a single legislator. (TODO: revisit this section later) There are two other roles that a page can play, we'll get into these in more detail in advanced topics. One is what we'll call an 'auxillary detail' page, essentially a subpage of a detail page that contains additional information we need to collect. (An example would be if each detail page had a subpage listing a legislator's committee assignments that we wanted to collect.) The final role is what we call 'augmentation' pages, which provide data with which we want to augment the entire data set. (An example would be a separate page that is a photo directory of all legislators. We'll need to scrape this and match the photos to the legislators we scraped from the listing/detail pages.) Why don't page objects fetch themselves? \u00b6 A big part of this is that we want to share some state among them for rate limiting and other configuration yet to come. In theory this could mean that each one is instantiated with a session parameter or similar, but that'd be a bit more cumbersome. This also can make testing even easier, as there's no need to mock HTTP requests/etc. as just passing a mock object into the Page will suffice.","title":"Design Philosophy"},{"location":"philosophy/#design-philosophy","text":"spatula came out of several years of maintaining a very large, community-driven scraper project, Open States . While several of the decisions made are best understood through this lens, the goal of spatula is to remain lightweight so that even simple projects can benefit from its structure.","title":"Design Philosophy"},{"location":"philosophy/#common-problems","text":"If you've written a few web scrapers, you're likely to have run into a few common issues: Scrapers are inherently messy as they tend to reflect the cleanliness (or lack thereof) of the underlying site. Scrapers are often written and then left alone for months or years, only needing maintenance when the underlying site changes. Some scrapes take hours to complete, making the development cycle for testing changes and fixes quite difficult.","title":"Common Problems"},{"location":"philosophy/#goals","text":"The framework should make it as easy as possible to get started and write clean scraper code. While it is often easy, and tempting, to write a scraper as a dirty one-off script, spatula makes an attempt to provide an easy framework that most scrapers fit within without additional overhead. This reflects the reality that many scraper projects start small but grow quickly, so reaching for a heavyweight tool from the start often does not seem practical. The initial overhead imposed by the framework should be as light as possible, providing benefits even for authors that do not wish to use every feature available to them. The framework should make it easy to read code that was written, with as many of the underlying assumptions of the scraper presented as clearly as possible for future maintenance. By encouraging users to structure their scrapers in a readable way, scrapers will be easier to read later, and issues/maintenance can often be identified with specific components instead of forcing maintainers to comprehend a single gnarly script. Iterative development of scrapers should be the norm. It should be possible to test changes to a single part of the scrape without running the entire process. The user shouldn't have to comment out code or add temporary debug statements to do something that is extremely common when authoring scrapers. Leverage modern Python to improve the experience. There are some great things about Python 3 that we can leverage to make writing & maintaining scrapers easier. To that end, spatula is fully type-annotated, and integrates well with dataclasses , attrs , and pydantic .","title":"Goals"},{"location":"philosophy/#pages-page-roles","text":"A key component of using spatula is thinking of the scraper in terms of types of \"pages\". For each type of page you encounter, you'll write a subclass of Page to extract the data from it. Note If you're familiar with MVC frameworks, a good way to think of this concept is the inverse of a view: a Page takes some kind of presentation (e.g. an HTML page or CSV file) and converts it back to the underlying data that it is comprised of. There are a few types of pages we generally encounter when scraping sites. How we write and use our Page subclasses will depend upon which of these four roles the page fulfills. The two most common are what we'll call \"listing\" and \"detail\" pages. Listing pages contain some kind of list of information, perhaps all of the members of a given legislature. They'll often provide links to our second type of page, which we'll call a \"detail\" page. A \"detail\" page in our example would contain additional information on a single legislator. (TODO: revisit this section later) There are two other roles that a page can play, we'll get into these in more detail in advanced topics. One is what we'll call an 'auxillary detail' page, essentially a subpage of a detail page that contains additional information we need to collect. (An example would be if each detail page had a subpage listing a legislator's committee assignments that we wanted to collect.) The final role is what we call 'augmentation' pages, which provide data with which we want to augment the entire data set. (An example would be a separate page that is a photo directory of all legislators. We'll need to scrape this and match the photos to the legislators we scraped from the listing/detail pages.)","title":"Pages &amp; Page Roles"},{"location":"philosophy/#why-dont-page-objects-fetch-themselves","text":"A big part of this is that we want to share some state among them for rate limiting and other configuration yet to come. In theory this could mean that each one is instantiated with a session parameter or similar, but that'd be a bit more cumbersome. This also can make testing even easier, as there's no need to mock HTTP requests/etc. as just passing a mock object into the Page will suffice.","title":"Why don't page objects fetch themselves?"},{"location":"reference/","text":"API Reference \u00b6 Pages \u00b6 Page \u00b6 Base class for all Page scrapers, used for scraping information from a single type of page. Attributes source Can be set on subclasses of Page to define the initial HTTP request that the page will handle in its process_response method. For simple GET requests, source can be a string. URL should be used for more advanced use cases. response requests.Response object available if access is needed to the raw response for any reason. dependencies TODO: document Methods get_next_source ( self ) \u00b6 To be overriden for paginated pages. Return a URL or valid source to fetch the next page, None if there isn't one. postprocess_response ( self ) \u00b6 To be overridden. This is called after source.get_response but before self.process_page. process_error_response ( self , exception ) \u00b6 To be overridden. This is called after source.get_response if an exception is raised. process_page ( self ) \u00b6 To be overridden. Return data extracted from this page and this page alone. HtmlPage \u00b6 Page that automatically handles parsing and normalizing links in an HTML response. Attributes root lxml.etree.Element object representing the root element (e.g. <html> ) on the page. Can use the normal lxml methods (such as cssselect and getchildren ), or use this element as the target of a Selector subclass. JsonPage \u00b6 Page that automatically handles parsing a JSON response. Attributes data JSON data from response. (same as self.response.json() ) PdfPage \u00b6 Page that automatically handles converting a PDF response to text using pdftotext . Attributes preserve_layout set to True on derived class if you want the conversion function to use pdftotext's -layout option to attempt to preserve the layout of text. ( False by default) text UTF8 text extracted by pdftotext. XmlPage \u00b6 Page that automatically handles parsing a XML response. Attributes root lxml.etree.Element object representing the root XML element on the page. ListPages \u00b6 ListPage \u00b6 Base class for common pattern of extracting many homogenous items from one page. Instead of overriding process_response , subclasses should provide a process_item . Methods process_item ( self , item ) \u00b6 To be overridden. Called once per subitem on page, as defined by the particular subclass being used. skip ( self , msg = '' ) \u00b6 Can be called from within process_item to skip a given item. Typically used if there is some known bad data. CsvListPage \u00b6 Processes each row in a CSV (after the first, assumed to be headers) as an item with process_item . ExcelListPage \u00b6 Processes each row in an Excel file as an item with process_item . HtmlListPage \u00b6 Selects homogenous items from HTML page using selector and passes them to process_item . Attributes selector Selector subclass which matches list of homogenous elements to process. (e.g. CSS(\"tbody tr\") ) JsonListPage \u00b6 Processes each element in a JSON list as an item with process_item . XmlListPage \u00b6 Selects homogenous items from XML document using selector and passes them to process_item . Attributes selector Selector subclass which matches list of homogenous elements to process. (e.g. XPath(\"//item\") ) Selectors \u00b6 Selector \u00b6 Base class implementing Selector interface. match ( self , element , * , min_items = None , max_items = None , num_items = None ) \u00b6 Return all matches of the given selector within element . If the number of elements matched is outside the prescribed boundaries, a SelectorError is raised. Parameters: Name Type Description Default element _Element The element to match within. When used from within a Page will usually be self.root . required min_items Optional[int] A minimum number of items to match. None max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None match_one ( self , element ) \u00b6 Return exactly one match. Parameters: Name Type Description Default element _Element Element to search within. required CSS \u00b6 Utilize CSS-style selectors. Parameters: Name Type Description Default css_selector str CSS selector expression to use. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None SimilarLink \u00b6 Match links that fit a provided pattern. Parameters: Name Type Description Default pattern str Regular expression for link hrefs. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None XPath \u00b6 Utilize XPath selectors. Parameters: Name Type Description Default xpath str XPath expression to use. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None Sources \u00b6 URL \u00b6 Defines a resource to fetch via URL, particularly useful for handling non-GET requests. Parameters: Name Type Description Default url str URL to fetch required method str HTTP method to use, defaults to \"GET\" 'GET' data dict POST data to include in request body. None headers dict dictionary of HTTP headers to set for the request. None NullSource \u00b6 Special class to set as a page's source to indicate no HTTP request needs to be performed. Workflows \u00b6 Workflow \u00b6 Define a complete workflow by which items can be scraped and saved to disk.","title":"API Reference"},{"location":"reference/#api-reference","text":"","title":"API Reference"},{"location":"reference/#pages","text":"","title":"Pages"},{"location":"reference/#page","text":"Base class for all Page scrapers, used for scraping information from a single type of page. Attributes source Can be set on subclasses of Page to define the initial HTTP request that the page will handle in its process_response method. For simple GET requests, source can be a string. URL should be used for more advanced use cases. response requests.Response object available if access is needed to the raw response for any reason. dependencies TODO: document Methods","title":"Page"},{"location":"reference/#spatula.pages.Page.get_next_source","text":"To be overriden for paginated pages. Return a URL or valid source to fetch the next page, None if there isn't one.","title":"get_next_source()"},{"location":"reference/#spatula.pages.Page.postprocess_response","text":"To be overridden. This is called after source.get_response but before self.process_page.","title":"postprocess_response()"},{"location":"reference/#spatula.pages.Page.process_error_response","text":"To be overridden. This is called after source.get_response if an exception is raised.","title":"process_error_response()"},{"location":"reference/#spatula.pages.Page.process_page","text":"To be overridden. Return data extracted from this page and this page alone.","title":"process_page()"},{"location":"reference/#htmlpage","text":"Page that automatically handles parsing and normalizing links in an HTML response. Attributes root lxml.etree.Element object representing the root element (e.g. <html> ) on the page. Can use the normal lxml methods (such as cssselect and getchildren ), or use this element as the target of a Selector subclass.","title":"HtmlPage"},{"location":"reference/#jsonpage","text":"Page that automatically handles parsing a JSON response. Attributes data JSON data from response. (same as self.response.json() )","title":"JsonPage"},{"location":"reference/#pdfpage","text":"Page that automatically handles converting a PDF response to text using pdftotext . Attributes preserve_layout set to True on derived class if you want the conversion function to use pdftotext's -layout option to attempt to preserve the layout of text. ( False by default) text UTF8 text extracted by pdftotext.","title":"PdfPage"},{"location":"reference/#xmlpage","text":"Page that automatically handles parsing a XML response. Attributes root lxml.etree.Element object representing the root XML element on the page.","title":"XmlPage"},{"location":"reference/#listpages","text":"","title":"ListPages"},{"location":"reference/#listpage","text":"Base class for common pattern of extracting many homogenous items from one page. Instead of overriding process_response , subclasses should provide a process_item . Methods","title":"ListPage"},{"location":"reference/#spatula.pages.ListPage.process_item","text":"To be overridden. Called once per subitem on page, as defined by the particular subclass being used.","title":"process_item()"},{"location":"reference/#spatula.pages.ListPage.skip","text":"Can be called from within process_item to skip a given item. Typically used if there is some known bad data.","title":"skip()"},{"location":"reference/#csvlistpage","text":"Processes each row in a CSV (after the first, assumed to be headers) as an item with process_item .","title":"CsvListPage"},{"location":"reference/#excellistpage","text":"Processes each row in an Excel file as an item with process_item .","title":"ExcelListPage"},{"location":"reference/#htmllistpage","text":"Selects homogenous items from HTML page using selector and passes them to process_item . Attributes selector Selector subclass which matches list of homogenous elements to process. (e.g. CSS(\"tbody tr\") )","title":"HtmlListPage"},{"location":"reference/#jsonlistpage","text":"Processes each element in a JSON list as an item with process_item .","title":"JsonListPage"},{"location":"reference/#xmllistpage","text":"Selects homogenous items from XML document using selector and passes them to process_item . Attributes selector Selector subclass which matches list of homogenous elements to process. (e.g. XPath(\"//item\") )","title":"XmlListPage"},{"location":"reference/#selectors","text":"","title":"Selectors"},{"location":"reference/#selector","text":"Base class implementing Selector interface.","title":"Selector"},{"location":"reference/#spatula.selectors.Selector.match","text":"Return all matches of the given selector within element . If the number of elements matched is outside the prescribed boundaries, a SelectorError is raised. Parameters: Name Type Description Default element _Element The element to match within. When used from within a Page will usually be self.root . required min_items Optional[int] A minimum number of items to match. None max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None","title":"match()"},{"location":"reference/#spatula.selectors.Selector.match_one","text":"Return exactly one match. Parameters: Name Type Description Default element _Element Element to search within. required","title":"match_one()"},{"location":"reference/#css","text":"Utilize CSS-style selectors. Parameters: Name Type Description Default css_selector str CSS selector expression to use. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None","title":"CSS"},{"location":"reference/#similarlink","text":"Match links that fit a provided pattern. Parameters: Name Type Description Default pattern str Regular expression for link hrefs. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None","title":"SimilarLink"},{"location":"reference/#xpath","text":"Utilize XPath selectors. Parameters: Name Type Description Default xpath str XPath expression to use. required min_items Optional[int] A minimum number of items to match. 1 max_items Optional[int] A maximum number of items to match. None num_items Optional[int] An exact number of items to match. None","title":"XPath"},{"location":"reference/#sources","text":"","title":"Sources"},{"location":"reference/#url","text":"Defines a resource to fetch via URL, particularly useful for handling non-GET requests. Parameters: Name Type Description Default url str URL to fetch required method str HTTP method to use, defaults to \"GET\" 'GET' data dict POST data to include in request body. None headers dict dictionary of HTTP headers to set for the request. None","title":"URL"},{"location":"reference/#nullsource","text":"Special class to set as a page's source to indicate no HTTP request needs to be performed.","title":"NullSource"},{"location":"reference/#workflows","text":"","title":"Workflows"},{"location":"reference/#workflow","text":"Define a complete workflow by which items can be scraped and saved to disk.","title":"Workflow"},{"location":"scraper-basics/","text":"A First Scraper \u00b6 This guide contains quick examples of how you could scrape a small list of employees of the fictional Yoyodyne Propulsion Systems , a site developed for demonstrating web scraping. This will give you an idea of what it looks like to write a scraper using spatula . Scraping a List Page \u00b6 It is fairly common for a scrape to begin on some sort of directory or listing page. We'll start by scraping the staff list on https://yoyodyne-propulsion.herokuapp.com/staff This page has a fairly simple HTML table with four columns: < table id = \"employees\" > < thead > < tr > < th > First Name </ th > < th > Last Name </ th > < th > Position Name </ th > < th > &nbsp; </ th > </ tr > </ thead > < tbody > < tr > < td > John </ td > < td > Barnett </ td > < td > Scheduling </ td > < td >< a href = \"/staff/52\" > Details </ a ></ td > </ tr > < tr > < td > John </ td > < td > Bigboot\u00e9 </ td > < td > Executive Vice President </ td > < td >< a href = \"/staff/2\" > Details </ a ></ td > </ tr > ...continues... spatula provides a special interface for these cases. See below how we process each matching link by deriving from a HtmlListPage and providing a selector as well as a process_item method. Open a file named quickstart.py and add the following code: # imports we'll use in this example from spatula import ( HtmlPage , HtmlListPage , CSS , XPath ) class EmployeeList ( HtmlListPage ): source = \"https://yoyodyne-propulsion.herokuapp.com/staff\" # each row represents an employee selector = CSS ( \"#employees tr\" ) def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return dict ( first = first . text , last = last . text , position = position . text , ) One concept in spatula is that we typically write one class per type of page we encounter. This class defines the logic to process the employee list page. This class will turn each row on the page into a dictionary with the 'first', 'last', and 'position' keys. It can be tested from the command line like: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: {'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} 2: {'first': 'John', 'last': 'Bigboot\u00e9', 'position': 'Executive Vice President'} 3: {'first': 'John', 'last': 'Camp', 'position': 'Human Resources'} ... 10: {'first': 'John', 'last': 'Fish', 'position': 'Marine R&D'} The spatula test command lets us quickly see the output of the part of the scraper we're working on. You may notice that we're only grabbing the first page for now, we'll come back in a bit to handle pagination. Scraping a Single Page \u00b6 Employees have a few more details not included in the table on pages like https://yoyodyne-propulsion.herokuapp.com/staff/52 . We're going to pull some data elements from the page that look like: < h2 class = \"section\" > Employee Details for John Barnett </ h2 > < div class = \"section\" > < dl > < dt > Position </ dt > < dd id = \"position\" > Scheduling </ dd > < dt > Marital Status </ dt > < dd id = \"status\" > Married </ dd > < dt > Number of Children </ dt > < dd id = \"children\" > 1 </ dd > < dt > Hired </ dt > < dd id = \"hired\" > 3/6/1963 </ dd > </ dl > </ div > To demonstrate extracting the details from this page, we'll write a small class to handle individual employee pages. Whereas before we used HtmlListPage and overrode process_item , this time we'll subclass HtmlPage , and override the process_page function. class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , ) This will extract the elements from the page and return them in a dictionary. It can be tested from the command line like: $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 {'children': '1', 'hired': '3/6/1963', 'marital_status': 'Married'} One thing to note is that since we didn't define a single source attribute like we did in EmployeeList , we need to pass one on the command line with --source . This lets you quickly try your scraper against multiple variants of a page as needed. Chaining Pages Together \u00b6 Most moderately complex sites will require chaining data together from multiple pages to get a complete object. Let's revisit EmployeeList and have it return instances of EmployeeDetail to tell spatula that more work is needed: class EmployeeList ( HtmlListPage ): # by providing this here, it can be omitted on the command line # useful in cases where the scraper is only meant for one page source = \"https://yoyodyne-propulsion.herokuapp.com/staff\" # each row represents an employee selector = CSS ( \"#employees tbody tr\" ) def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( dict ( first = first . text , last = last . text , position = position . text , ), source = XPath ( \"./a/@href\" ) . match_one ( details ), ) And we can revisit EmployeeDetail to tell it to combine the data it collects with the data passed in from the parent page: class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape, # in this case a dict we can expand here ** self . input , ) Now a run looks like: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: EmployeeDetail(input={'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} source=https://yoyodyne-propulsion.herokuapp.com/staff) 2: EmployeeDetail(input={'first': 'John', 'last': 'Bigboot\u00e9', 'position': 'Executive Vice President'} source=https:/yoyodyne-propulsion.herokuapp.com/staff/2) ... 10: EmployeeDetail(input={'first': 'John', 'last': 'Fish', 'position': 'Marine R&D'} source=https:/yoyodyne-propulsion.herokuapp.com/staff/20) By default, spatula test just shows the result of the page you're working on, but you can see that it is now returning page objects with the data and a source set. Running a Scrape \u00b6 Now that we're happy with our individual page scrapers, we can run the full scrape and write the data to disk. For this we use the spatula scrape command: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/2 ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/100 INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/101 success: wrote 10 objects to _scrapes/2021-06-03/001 And now our scraped data is on disk, ready for you to use! If you look at a data file you'll see that it has the full data for an individual: { \"marital_status\" : \"Single\" , \"children\" : \"0\" , \"hired\" : \"9/9/1963\" , \"first\" : \"John\" , \"last\" : \"Omar\" , \"position\" : \"Imports & Exports\" } In Next Steps we'll take a look at how to handle pagination, error handling, and validating scraped data.","title":"A First Scraper"},{"location":"scraper-basics/#a-first-scraper","text":"This guide contains quick examples of how you could scrape a small list of employees of the fictional Yoyodyne Propulsion Systems , a site developed for demonstrating web scraping. This will give you an idea of what it looks like to write a scraper using spatula .","title":"A First Scraper"},{"location":"scraper-basics/#scraping-a-list-page","text":"It is fairly common for a scrape to begin on some sort of directory or listing page. We'll start by scraping the staff list on https://yoyodyne-propulsion.herokuapp.com/staff This page has a fairly simple HTML table with four columns: < table id = \"employees\" > < thead > < tr > < th > First Name </ th > < th > Last Name </ th > < th > Position Name </ th > < th > &nbsp; </ th > </ tr > </ thead > < tbody > < tr > < td > John </ td > < td > Barnett </ td > < td > Scheduling </ td > < td >< a href = \"/staff/52\" > Details </ a ></ td > </ tr > < tr > < td > John </ td > < td > Bigboot\u00e9 </ td > < td > Executive Vice President </ td > < td >< a href = \"/staff/2\" > Details </ a ></ td > </ tr > ...continues... spatula provides a special interface for these cases. See below how we process each matching link by deriving from a HtmlListPage and providing a selector as well as a process_item method. Open a file named quickstart.py and add the following code: # imports we'll use in this example from spatula import ( HtmlPage , HtmlListPage , CSS , XPath ) class EmployeeList ( HtmlListPage ): source = \"https://yoyodyne-propulsion.herokuapp.com/staff\" # each row represents an employee selector = CSS ( \"#employees tr\" ) def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return dict ( first = first . text , last = last . text , position = position . text , ) One concept in spatula is that we typically write one class per type of page we encounter. This class defines the logic to process the employee list page. This class will turn each row on the page into a dictionary with the 'first', 'last', and 'position' keys. It can be tested from the command line like: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: {'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} 2: {'first': 'John', 'last': 'Bigboot\u00e9', 'position': 'Executive Vice President'} 3: {'first': 'John', 'last': 'Camp', 'position': 'Human Resources'} ... 10: {'first': 'John', 'last': 'Fish', 'position': 'Marine R&D'} The spatula test command lets us quickly see the output of the part of the scraper we're working on. You may notice that we're only grabbing the first page for now, we'll come back in a bit to handle pagination.","title":"Scraping a List Page"},{"location":"scraper-basics/#scraping-a-single-page","text":"Employees have a few more details not included in the table on pages like https://yoyodyne-propulsion.herokuapp.com/staff/52 . We're going to pull some data elements from the page that look like: < h2 class = \"section\" > Employee Details for John Barnett </ h2 > < div class = \"section\" > < dl > < dt > Position </ dt > < dd id = \"position\" > Scheduling </ dd > < dt > Marital Status </ dt > < dd id = \"status\" > Married </ dd > < dt > Number of Children </ dt > < dd id = \"children\" > 1 </ dd > < dt > Hired </ dt > < dd id = \"hired\" > 3/6/1963 </ dd > </ dl > </ div > To demonstrate extracting the details from this page, we'll write a small class to handle individual employee pages. Whereas before we used HtmlListPage and overrode process_item , this time we'll subclass HtmlPage , and override the process_page function. class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , ) This will extract the elements from the page and return them in a dictionary. It can be tested from the command line like: $ spatula test quickstart.EmployeeDetail --source \"https://yoyodyne-propulsion.herokuapp.com/staff/52\" INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 {'children': '1', 'hired': '3/6/1963', 'marital_status': 'Married'} One thing to note is that since we didn't define a single source attribute like we did in EmployeeList , we need to pass one on the command line with --source . This lets you quickly try your scraper against multiple variants of a page as needed.","title":"Scraping a Single Page"},{"location":"scraper-basics/#chaining-pages-together","text":"Most moderately complex sites will require chaining data together from multiple pages to get a complete object. Let's revisit EmployeeList and have it return instances of EmployeeDetail to tell spatula that more work is needed: class EmployeeList ( HtmlListPage ): # by providing this here, it can be omitted on the command line # useful in cases where the scraper is only meant for one page source = \"https://yoyodyne-propulsion.herokuapp.com/staff\" # each row represents an employee selector = CSS ( \"#employees tbody tr\" ) def process_item ( self , item ): # this function is called for each <tr> we get from the selector # we know there are 4 <tds> first , last , position , details = item . getchildren () return EmployeeDetail ( dict ( first = first . text , last = last . text , position = position . text , ), source = XPath ( \"./a/@href\" ) . match_one ( details ), ) And we can revisit EmployeeDetail to tell it to combine the data it collects with the data passed in from the parent page: class EmployeeDetail ( HtmlPage ): def process_page ( self ): marital_status = CSS ( \"#status\" ) . match_one ( self . root ) children = CSS ( \"#children\" ) . match_one ( self . root ) hired = CSS ( \"#hired\" ) . match_one ( self . root ) return dict ( marital_status = marital_status . text , children = children . text , hired = hired . text , # self.input is the data passed in from the prior scrape, # in this case a dict we can expand here ** self . input , ) Now a run looks like: $ spatula test quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff 1: EmployeeDetail(input={'first': 'John', 'last': 'Barnett', 'position': 'Scheduling'} source=https://yoyodyne-propulsion.herokuapp.com/staff) 2: EmployeeDetail(input={'first': 'John', 'last': 'Bigboot\u00e9', 'position': 'Executive Vice President'} source=https:/yoyodyne-propulsion.herokuapp.com/staff/2) ... 10: EmployeeDetail(input={'first': 'John', 'last': 'Fish', 'position': 'Marine R&D'} source=https:/yoyodyne-propulsion.herokuapp.com/staff/20) By default, spatula test just shows the result of the page you're working on, but you can see that it is now returning page objects with the data and a source set.","title":"Chaining Pages Together"},{"location":"scraper-basics/#running-a-scrape","text":"Now that we're happy with our individual page scrapers, we can run the full scrape and write the data to disk. For this we use the spatula scrape command: $ spatula scrape quickstart.EmployeeList INFO:quickstart.EmployeeList:fetching https://yoyodyne-propulsion.herokuapp.com/staff INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/52 INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/2 ... INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/100 INFO:quickstart.EmployeeDetail:fetching https://yoyodyne-propulsion.herokuapp.com/staff/101 success: wrote 10 objects to _scrapes/2021-06-03/001 And now our scraped data is on disk, ready for you to use! If you look at a data file you'll see that it has the full data for an individual: { \"marital_status\" : \"Single\" , \"children\" : \"0\" , \"hired\" : \"9/9/1963\" , \"first\" : \"John\" , \"last\" : \"Omar\" , \"position\" : \"Imports & Exports\" } In Next Steps we'll take a look at how to handle pagination, error handling, and validating scraped data.","title":"Running a Scrape"}]}